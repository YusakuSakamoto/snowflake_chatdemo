{"metrics":[{"as_of_at":"2026-01-02 00:39:11.705 -0800","column":"DOC_ID","metrics":{"distinct_count":38,"distinct_rate_non_null":1.000000000000000e+00,"max_len":32,"max_varchar":"ffea89f03639a3ea11707e52a3097ab0","min_len":32,"min_varchar":"0427f235a3c0ef36644b00f98b855af1","null_count":0,"null_rate":0.000000000000000e+00,"row_count":38,"top_values":[{"count":1,"value":"a98f2c0723879c2c33716737bd9dfeb6"},{"count":1,"value":"050fe1c12cfc8ef6de612d9efaa1269e"},{"count":1,"value":"748e428c6963ae194fe7952c0bfdace1"},{"count":1,"value":"e98778f934e88e6982a48429b9098d34"},{"count":1,"value":"eeb5082b1aecf95a0393b294eeb6813e"},{"count":1,"value":"107eed0271693169875670491efaa64c"},{"count":1,"value":"1838e9516da5a272d9a387443662bfff"},{"count":1,"value":"43f3dc61536a9e8b923189e5dc995578"},{"count":1,"value":"9bfe8170b0df8cf12a2625672677dbb6"},{"count":1,"value":"431f6a8b9a844e4e93e3952a053bcf14"},{"count":1,"value":"062b6329ade104659536899a69cff377"},{"count":1,"value":"0a8c984d5cb5f9f416684ee6a0d7122e"},{"count":1,"value":"4c15dfe732fac9e32fa05b67b05b2f89"},{"count":1,"value":"51efc7e06ce154cb8d8c31771f91efcf"},{"count":1,"value":"6241698e356e0a166f39072196be6c55"},{"count":1,"value":"8d08e96f79326e95c3f2586230f1003c"},{"count":1,"value":"812a630b7eff40b5856ecebdb9ebdf5e"},{"count":1,"value":"89df2b102305b1b856fb9dba3abae683"},{"count":1,"value":"72fb48b0f6325a410b918c96965fe737"},{"count":1,"value":"615aa341799473308947f918b503ebf0"}]},"run_id":"RUN-3502f0d4-014a-46fc-bf64-d5733286f9d0"},{"as_of_at":"2026-01-02 00:39:12.815 -0800","column":"CONTENT","metrics":{"distinct_count":34,"distinct_rate_non_null":1.000000000000000e+00,"max_len":42776,"max_varchar":"run_id: RUN-021612e8-9e32-41f8-af96-40191ccb8d48\\ntarget_db: GBPS253YS_DB\\ntarget_schema: DB_DESIGN\\ntarget_table: PROFILE_RESULTS\\ntype: profile_evidence\\n| `AS_OF_AT` | 0% | 13 |\\n| `METRICS` | 0% | 8 |\\n| `RUN_ID` | 0% | 7 |\\n| `TARGET_COLUMN` | 0% | 17 |\\n| `TARGET_DB` | 0% | 1 |\\n| `TARGET_SCHEMA` | 0% | 2 |\\n| `TARGET_TABLE` | 0% | 5 |\\n| column | null_rate | distinct_count |\\n|---|---:|---:|\\n \\n \\n \\n# Profile Evidence: DB_DESIGN.PROFILE_RESULTS\\n## Columns (summary)\\n## Raw metrics\\n- File: `reviews/profiles/20251229_140447305/DB_DESIGN/PROFILE_RESULTS.raw.json_0_0_0`\\n- Prefix: `reviews/profiles/20251229_140447305/DB_DESIGN/PROFILE_RESULTS.raw.json`\\n---\\n---\\nas_of_at: 2025-12-30T05:06:10\\ngenerated_on: 20251229_140447305\\nrow_count: 19\\n","min_len":103,"min_varchar":"# DDL Generator\\n\\nこのファイルは、Snowflakeの各種DDLを一括生成するための統合ビューです。\\n\\n## 事前準備：DB・WHの作成\\n\\nDDL生成の前に、まずデータベースとウェアハウスを作成してください。\\n\\n```sql\\nUSE ROLE ACCOUNTADMIN;\\nCREATE DATABASE IF NOT EXISTS GBPS253YS_DB;\\nCREATE OR REPLACE WAREHOUSE GBPS253YS_WH WITH\\n     WAREHOUSE_SIZE='X-SMALL'\\n     AUTO_SUSPEND = 120\\n     AUTO_RESUME = TRUE\\n     INITIALLY_SUSPENDED=TRUE;\\n     \\nUSE DATABASE GBPS253YS_DB;\\nUSE WAREHOUSE GBPS253YS_WH;\\n\\nDROP SCHEMA PUBLIC;\\n```\\n\\n---\\n\\n## 生成可能なDDL\\n\\n1. **Snowflake DDL Generator** - スキーマ、テーブル、ビュー、プロシージャなど（セマンティックビューはコメントでYAML出力）\\n2. **External Tables DDL Generator** - S3/Azure連携の外部テーブル\\n3. **YAML FILE Generator** - セマンティックビューのYAMLファイル生成\\n\\n> **Note:** セマンティックビュー（YAML）は、Snowflake DDL Generatorではコメント形式で出力されます。  \\n> 実際のYAMLファイルとして出力する場合は、YAML FILE Generatorを使用してください。\\n\\n---\\n\\n## 1. Snowflake DDL Generator\\n\\n```dataviewjs\\n(async () => {\\n  // ==============================\\n  // Snowflake DDL Generator (FAST + COPY + FILE OUTPUT)\\n  // ==============================\\n\\n  // paths\\n  const SCHEMAS_PATH = \\"master/schemas\\";\\n  const TABLES_PATH  = \\"master/tables\\";\\n  const COLUMNS_PATH = \\"master/columns\\";\\n  const VIEWS_PATH   = \\"master/views\\";\\n  const SEMANTIC_VIEWS_PATH = \\"master/semanticviews\\";\\n  const OTHER_PATH   = \\"master/other\\";\\n\\n  // options\\n  const QUOTE_IDENT     = false;\\n  const EMIT_COMMENTS   = true;\\n  const IO_CONCURRENCY  = 8;     // VIEW/OTHER の md 読み込み並列数（増やしすぎ注意）\\n  const RUN_ON_LOAD     = false; // true にするとノート表示時に自動生成（重いなら false 推奨）\\n\\n  // output\\n  const OUTPUT_FOLDER   = \\"generated/ddl\\";           // Vault 内フォルダ\\n  const OUTPUT_PREFIX   = \\"snowflake_ddl\\";           // 例: snowflake_ddl_20251227_235959.sql\\n  const WRITE_SQL_FILE  = true;                       // Vault へ .sql 出力\\n\\n  // ==============================\\n  // UI\\n  // ==============================\\n  dv.header(2, \\"Snowflake DDL Generator\\");\\n  const statusEl = dv.el(\\"div\\", \\"準備完了（Generate を押してください）\\");\\n\\n  const btnRow = dv.el(\\"div\\", \\"\\");\\n  btnRow.style.display = \\"flex\\";\\n  btnRow.style.gap = \\"8px\\";\\n  btnRow.style.flexWrap = \\"wrap\\";\\n\\n  const btnGenerate = document.createElement(\\"button\\");\\n  btnGenerate.textContent = \\"Generate\\";\\n  btnRow.appendChild(btnGenerate);\\n\\n  const btnCopy = document.createElement(\\"button\\");\\n  btnCopy.textContent = \\"Copy to Clipboard\\";\\n  btnCopy.disabled = true;\\n  btnRow.appendChild(btnCopy);\\n\\n  const btnOpenFile = document.createElement(\\"button\\");\\n  btnOpenFile.textContent = \\"Open Output File\\";\\n  btnOpenFile.disabled = true;\\n  btnRow.appendChild(btnOpenFile);\\n\\n  // Preview (textarea)\\n  const details = dv.el(\\"details\\", \\"\\");\\n  const summary = document.createElement(\\"summary\\");\\n  summary.textContent = \\"Preview（textarea：ここから手動コピーも可）\\";\\n  details.appendChild(summary);\\n\\n  const ta = document.createElement(\\"textarea\\");\\n  ta.readOnly = true;\\n  ta.style.width = \\"100%\\";\\n  ta.style.height = \\"360px\\";\\n  ta.style.fontFamily = \\"monospace\\";\\n  ta.placeholder = \\"Generate 後にここに DDL を表示します\\";\\n  details.appendChild(ta);\\n\\n  // state\\n  let lastDDL = \\"\\";\\n  let lastFilePath = \\"\\";\\n\\n  // ==============================\\n  // helpers\\n  // ==============================\\n  function q(name) {\\n    if (!QUOTE_IDENT) return name;\\n    return `\\"${String(name).replaceAll('\\"', '\\"\\"')}\\"`;\\n  }\\n  function clean(v) {\\n    return (v === null || v === undefined) ? \\"\\" : String(v);\\n  }\\n  function bool(v, def=false) {\\n    if (v === true || v === false) return v;\\n    if (typeof v === \\"string\\") return v.toLowerCase() === \\"true\\";\\n    return def;\\n  }\\n  function sqlStringLiteral(s) {\\n    return \\"'\\" + String(s).replaceAll(\\"'\\", \\"''\\") + \\"'\\";\\n  }\\n  function key2(schemaPhysical, tablePhysical) {\\n    return `${clean(schemaPhysical).toUpperCase()}|${clean(tablePhysical).toUpperCase()}`;\\n  }\\n  function hasDefault(v) {\\n    if (v === null || v === undefined) return false;\\n    const s = String(v).trim();\\n    if (s === \\"\\" || s.toLowerCase() === \\"null\\") return false;\\n    return true;\\n  }\\n  function normalizeLF(s){\\n    return String(s ?? \\"\\").replace(/\\\\r\\\\n/g, \\"\\\\n\\").replace(/\\\\r/g, \\"\\\\n\\");\\n  }\\n  function parseSqlFromBody(md){\\n    md = normalizeLF(md);\\n    const m = md.match(/```sql\\\\s*([\\\\s\\\\S]*?)```/i);\\n    return m ? (m[1] ?? \\"\\").trim() : \\"\\";\\n  }\\n\\n  function parseViewColumnsFromBody(md) {\\n    md = normalizeLF(md);\\n    const lines = md.split(\\"\\\\n\\");\\n\\n    let headerIdx = -1;\\n    for (let i = 0; i < lines.length; i++) {\\n      const ln = lines[i].trim();\\n      if (!ln.includes(\\"|\\")) continue;\\n      const cells = ln.split(\\"|\\").map(s => s.trim()).filter(Boolean);\\n      const lower = cells.map(c => c.toLowerCase());\\n      if (\\n        (lower.includes(\\"column_name\\") || lower.includes(\\"column\\") || lower.includes(\\"name\\")) &&\\n        (lower.includes(\\"comment\\") || lower.includes(\\"description\\"))\\n      ) {\\n        headerIdx = i;\\n        break;\\n      }\\n    }\\n    if (headerIdx < 0) return [];\\n\\n    let sepIdx = -1;\\n    for (let i = headerIdx + 1; i < Math.min(lines.length, headerIdx + 6); i++) {\\n      const ln = lines[i].trim();\\n      if (ln.includes(\\"|\\") && /-[-\\\\s|:]+-/.test(ln)) { sepIdx = i; break; }\\n    }\\n    if (sepIdx < 0) return [];\\n\\n    const headerCells = lines[headerIdx].split(\\"|\\").map(s => s.trim()).filter(Boolean);\\n    const lowerHeader = headerCells.map(c => c.toLowerCase());\\n    const colIdx = lowerHeader.findIndex(h => [\\"column_name\\",\\"column\\",\\"name\\"].includes(h));\\n    const cmtIdx = lowerHeader.findIndex(h => [\\"comment\\",\\"description\\"].includes(h));\\n    const cIdx = colIdx >= 0 ? colIdx : 0;\\n    const dIdx = cmtIdx >= 0 ? cmtIdx : 1;\\n\\n    const out = [];\\n    for (let i = sepIdx + 1; i < lines.length; i++) {\\n      const ln = lines[i].trim();\\n      if (!ln.includes(\\"|\\")) break;\\n      if (ln === \\"\\") break;\\n      const cells = lines[i].split(\\"|\\").map(s => s.trim()).filter(Boolean);\\n      const name = (cells[cIdx] ?? \\"\\").trim();\\n      const comment = (cells[dIdx] ?? \\"\\").trim();\\n      if (!name) continue;\\n      out.push({ name, comment });\\n    }\\n    return out;\\n  }\\n\\n  function ts() {\\n    const d = new Date();\\n    const pad = (n) => String(n).padStart(2, \\"0\\");\\n    return `${d.getFullYear()}${pad(d.getMonth()+1)}${pad(d.getDate())}_${pad(d.getHours())}${pad(d.getMinutes())}${pad(d.getSeconds())}`;\\n  }\\n\\n  async function ensureFolder(folderPath) {\\n    const parts = folderPath.split(\\"/\\").filter(Boolean);\\n    let cur = \\"\\";\\n    for (const p of parts) {\\n      cur = cur ? `${cur}/${p}` : p;\\n      if (!app.vault.getAbstractFileByPath(cur)) {\\n        try { await app.vault.createFolder(cur); } catch (e) { /* ignore */ }\\n      }\\n    }\\n  }\\n\\n  async function writeFile(path, content) {\\n    const af = app.vault.getAbstractFileByPath(path);\\n    if (af && af.children) throw new Error(`出力先がフォルダです: ${path}`);\\n    if (af) {\\n      await app.vault.modify(af, content);\\n    } else {\\n      await app.vault.create(path, content);\\n    }\\n    return app.vault.getAbstractFileByPath(path);\\n  }\\n\\n  async function openFileByPath(path) {\\n    const af = app.vault.getAbstractFileByPath(path);\\n    if (!af) throw new Error(`ファイルが見つかりません: ${path}`);\\n    if (af.children) throw new Error(`フォルダは開けません: ${path}`);\\n    await app.workspace.getLeaf(true).openFile(af);\\n  }\\n\\n  async function copyToClipboard(text) {\\n    // クリック操作中に呼ばれる想定（Clipboard API 成功率UP）\\n    try {\\n      await navigator.clipboard.writeText(text);\\n      return true;\\n    } catch (e) {\\n      // fallback: textarea を選択して execCommand\\n      try {\\n        ta.value = text;\\n        ta.focus();\\n        ta.select();\\n        const ok = document.execCommand(\\"copy\\");\\n        ta.setSelectionRange(0, 0);\\n        return !!ok;\\n      } catch (e2) {\\n        return false;\\n      }\\n    }\\n  }\\n\\n  async function mapLimit(items, limit, fn) {\\n    const results = new Array(items.length);\\n    let next = 0;\\n    const workers = Array.from({ length: Math.max(1, limit) }, async () => {\\n      while (true) {\\n        const i = next++;\\n        if (i >= items.length) break;\\n        results[i] = await fn(items[i], i);\\n      }\\n    });\\n    await Promise.all(workers);\\n    return results;\\n  }\\n\\n  // ==============================\\n  // main generator\\n  // ==============================\\n  async function generateDDL() {\\n    const warns = [];\\n\\n    // ---- load schemas ----\\n    const schemas = dv.pages(`\\"${SCHEMAS_PATH}\\"`)\\n      .where(p => p.schema_id && p.physical)\\n      .array()\\n      .sort((a,b)=>clean(a.physical).localeCompare(clean(b.physical)));\\n\\n    const schemaIdToPhysical = new Map(schemas.map(s => [String(s.schema_id), s.physical]));\\n\\n    // ---- load tables ----\\n    const tables = dv.pages(`\\"${TABLES_PATH}\\"`)\\n      .where(p => p.table_id && p.schema_id && p.physical)\\n      .array();\\n\\n    const tableIdToInfo = new Map();\\n    for (const t of tables) {\\n      const sch = schemaIdToPhysical.get(String(t.schema_id));\\n      if (!sch) {\\n        warns.push(`table_id=${t.table_id} unknown schema_id=${t.schema_id}`);\\n        continue;\\n      }\\n      tableIdToInfo.set(String(t.table_id), { schemaPhysical: sch, tablePhysical: t.physical, table: t });\\n    }\\n\\n    // ---- load columns ----\\n    const cols = dv.pages(`\\"${COLUMNS_PATH}\\"`)\\n      .where(p => p.column_id && p.table_id && p.physical)\\n      .array();\\n\\n    const colsByFqn = new Map();\\n    for (const c of cols) {\\n      const info = tableIdToInfo.get(String(c.table_id));\\n      if (!info) {\\n        warns.push(`column_id=${c.column_id} unknown table_id=${c.table_id}`);\\n        continue;\\n      }\\n      const k = key2(info.schemaPhysical, info.tablePhysical);\\n      if (!colsByFqn.has(k)) colsByFqn.set(k, []);\\n      colsByFqn.get(k).push(c);\\n    }\\n\\n    for (const arr of colsByFqn.values()) {\\n      arr.sort((a,b)=>{\\n        if (bool(a.pk) !== bool(b.pk)) return bool(a.pk) ? -1 : 1;\\n        return clean(a.physical).localeCompare(clean(b.physical));\\n      });\\n    }\\n\\n    // ---- load views / semantic views / other ----\\n    const views = dv.pages(`\\"${VIEWS_PATH}\\"`)\\n      .where(p => p.view_id && p.schema_id && p.physical)\\n      .array();\\n\\n    const semanticViews = dv.pages(`\\"${SEMANTIC_VIEWS_PATH}\\"`)\\n      .where(p => p.type === \\"semantic_view\\" && p.physical)\\n      .array();\\n\\n    const others = dv.pages(`\\"${OTHER_PATH}\\"`)\\n      .where(p => p.type === \\"other\\" && p.schema_id)\\n      .array();\\n\\n    // ==============================\\n    // build DDL (FAST: array push -> join)\\n    // ==============================\\n    const out = [];\\n\\n    // ---- SCHEMA ----\\n    for (const s of schemas) {\\n      out.push(`CREATE OR REPLACE SCHEMA ${q(s.physical)};\\\\n`);\\n      if (EMIT_COMMENTS && clean(s.comment)) {\\n        out.push(`COMMENT ON SCHEMA ${q(s.physical)} IS ${sqlStringLiteral(s.comment)};\\\\n`);\\n      }\\n      out.push(\\"\\\\n\\");\\n    }\\n\\n    // ---- TABLE ----\\n    for (const t of tables) {\\n      const sch = schemaIdToPhysical.get(String(t.schema_id));\\n      if (!sch) continue;\\n\\n      const fqtn = `${q(sch)}.${q(t.physical)}`;\\n      const colsArr = colsByFqn.get(key2(sch, t.physical)) ?? [];\\n      if (colsArr.length === 0) {\\n        out.push(`-- SKIP: ${fqtn} (no columns)\\\\n\\\\n`);\\n        continue;\\n      }\\n\\n      const lines = [];\\n      const pkCols = [];\\n\\n      for (const c of colsArr) {\\n        const notNull = bool(c.is_nullable, true) ? \\"\\" : \\" NOT NULL\\";\\n        const defSql  = hasDefault(c.default) ? ` DEFAULT ${String(c.default).trim()}` : \\"\\";\\n        lines.push(`  ${q(c.physical)} ${clean(c.domain)||\\"VARCHAR\\"}${defSql}${notNull}`);\\n        if (bool(c.pk)) pkCols.push(q(c.physical));\\n      }\\n      if (pkCols.length) lines.push(`  PRIMARY KEY (${pkCols.join(\\", \\")})`);\\n\\n      out.push(`CREATE OR REPLACE TABLE ${fqtn} (\\\\n${lines.join(\\",\\\\n\\")}\\\\n);\\\\n`);\\n      if (EMIT_COMMENTS && clean(t.comment)) {\\n        out.push(`COMMENT ON TABLE ${fqtn} IS ${sqlStringLiteral(t.comment)};\\\\n`);\\n      }\\n      if (EMIT_COMMENTS) {\\n        for (const c of colsArr) {\\n          if (!clean(c.comment)) continue;\\n          out.push(`COMMENT ON COLUMN ${fqtn}.${q(c.physical)} IS ${sqlStringLiteral(c.comment)};\\\\n`);\\n        }\\n      }\\n      out.push(\\"\\\\n\\");\\n    }\\n\\n    // ---- VIEW ----\\n    out.push(`-- ==============================\\\\n-- VIEWS\\\\n-- ==============================\\\\n`);\\n\\n    const viewBlocks = await mapLimit(views, IO_CONCURRENCY, async (v) => {\\n      const sch = schemaIdToPhysical.get(String(v.schema_id));\\n      if (!sch) {\\n        return { ddl: \\"\\", warn: `view_id=${v.view_id} unknown schema_id=${v.schema_id}` };\\n      }\\n\\n      let md = \\"\\";\\n      try { md = await dv.io.load(v.file.path); }\\n      catch { return { ddl: \\"\\", warn: `view_id=${v.view_id} load failed` }; }\\n\\n      const vcols = parseViewColumnsFromBody(md);\\n      const sql = parseSqlFromBody(md);\\n\\n      if (!vcols.length || !sql) {\\n        return { ddl: `-- SKIP: ${sch}.${v.physical} (missing columns or sql)\\\\n\\\\n`, warn: \\"\\" };\\n      }\\n\\n      const header =\\n        `CREATE OR REPLACE VIEW ${q(sch)}.${q(v.physical)} (\\\\n` +\\n        vcols.map(c =>\\n          clean(c.comment)\\n            ? `  ${q(c.name)} COMMENT ${sqlStringLiteral(c.comment)}`\\n            : `  ${q(c.name)}`\\n        ).join(\\",\\\\n\\") +\\n        `\\\\n)`;\\n\\n      const viewComment = (EMIT_COMMENTS && clean(v.comment))\\n        ? ` COMMENT = ${sqlStringLiteral(v.comment)}`\\n        : \\"\\";\\n\\n      const ddl = `${header}${viewComment}\\\\nAS\\\\n${sql};\\\\n\\\\n`;\\n      return { ddl, warn: \\"\\" };\\n    });\\n\\n    for (const r of viewBlocks) {\\n      if (r?.warn) warns.push(r.warn);\\n      if (r?.ddl) out.push(r.ddl);\\n    }\\n\\n    // ---- SEMANTIC VIEWS (YAML) ----\\n    out.push(`-- ==============================\\\\n-- SEMANTIC VIEWS (YAML)\\\\n-- ==============================\\\\n`);\\n\\n    const semanticViewBlocks = await mapLimit(semanticViews, IO_CONCURRENCY, async (sv) => {\\n      let md = \\"\\";\\n      try { md = await dv.io.load(sv.file.path); }\\n      catch { return { ddl: \\"\\", warn: `semantic_view ${sv.physical} load failed` }; }\\n\\n      // YAMLブロックを抽出\\n      const yamlMatch = md.match(/```yaml\\\\s*([\\\\s\\\\S]*?)```/i);\\n      if (!yamlMatch) {\\n        return { ddl: `-- SKIP: ${sv.physical} (no yaml block)\\\\n\\\\n`, warn: \\"\\" };\\n      }\\n\\n      const yamlContent = yamlMatch[1].trim();\\n      const comment = clean(sv.comment) || `Semantic View: ${sv.physical}`;\\n\\n      const ddl =\\n        `-- SEMANTIC VIEW: ${sv.physical}\\\\n` +\\n        `-- ${comment}\\\\n` +\\n        `-- ファイル出力先: ${sv.physical}.yaml\\\\n` +\\n        `/*\\\\n${yamlContent}\\\\n*/\\\\n\\\\n`;\\n      \\n      return { ddl, warn: \\"\\" };\\n    });\\n\\n    for (const r of semanticViewBlocks) {\\n      if (r?.warn) warns.push(r.warn);\\n      if (r?.ddl) out.push(r.ddl);\\n    }\\n\\n    // ---- OTHER ----\\n    out.push(`-- ==============================\\\\n-- OTHER OBJECTS\\\\n-- ==============================\\\\n`);\\n\\n    const otherBlocks = await mapLimit(others, IO_CONCURRENCY, async (o) => {\\n      const sch = schemaIdToPhysical.get(String(o.schema_id));\\n      if (!sch) {\\n        return { ddl: \\"\\", warn: `other ${o.file?.path} unknown schema_id=${o.schema_id}` };\\n      }\\n\\n      let md = \\"\\";\\n      try { md = await dv.io.load(o.file.path); }\\n      catch { return { ddl: \\"\\", warn: `other ${o.file?.path} load failed` }; }\\n\\n      const sql = parseSqlFromBody(md);\\n      if (!sql) {\\n        return { ddl: \\"\\", warn: `other ${o.file?.path} has no sql block` };\\n      }\\n\\n      const ddl =\\n        `-- ${o.object_type?.toUpperCase() ?? \\"OTHER\\"}: ${sch}.${o.physical ?? \\"\\"}\\\\n` +\\n        `${sql}\\\\n\\\\n`;\\n      return { ddl, warn: \\"\\" };\\n    });\\n\\n    for (const r of otherBlocks) {\\n      if (r?.warn) warns.push(r.warn);\\n      if (r?.ddl) out.push(r.ddl);\\n    }\\n\\n    // ---- WARNINGS ----\\n    if (warns.length) {\\n      out.push(`-- ==============================\\\\n-- WARNINGS\\\\n-- ==============================\\\\n`);\\n      for (const w of warns) out.push(`-- ${w}\\\\n`);\\n      out.push(\\"\\\\n\\");\\n    }\\n\\n    const ddl = out.join(\\"\\").trimEnd() + \\"\\\\n\\";\\n    return { ddl, warns };\\n  }\\n\\n  // ==============================\\n  // actions\\n  // ==============================\\n  btnGenerate.addEventListener(\\"click\\", async () => {\\n    btnGenerate.disabled = true;\\n    btnCopy.disabled = true;\\n    btnOpenFile.disabled = true;\\n    statusEl.textContent = \\"生成中…（VIEW/OTHER が多いと少し待ちます）\\";\\n    ta.value = \\"\\";\\n\\n    try {\\n      const t0 = performance.now();\\n      const { ddl, warns } = await generateDDL();\\n      const t1 = performance.now();\\n\\n      lastDDL = ddl;\\n\\n      // write DDL file\\n      if (WRITE_SQL_FILE) {\\n        await ensureFolder(OUTPUT_FOLDER);\\n        lastFilePath = `${OUTPUT_FOLDER}/${OUTPUT_PREFIX}_${ts()}.sql`;\\n        await writeFile(lastFilePath, ddl);\\n        btnOpenFile.disabled = false;\\n      }\\n\\n      // preview\\n      ta.value = ddl;\\n\\n      btnCopy.disabled = false;\\n      statusEl.textContent =\\n        `完了: ${Math.round(t1 - t0)} ms / ${ddl.length.toLocaleString()} chars` +\\n        (WRITE_SQL_FILE ? ` / 出力: ${lastFilePath}` : \\"\\") +\\n        (warns.length ? ` / WARNINGS: ${warns.length}` : \\"\\");\\n    } catch (e) {\\n      console.error(e);\\n      statusEl.textContent = `失敗: ${e?.message ?? e}`;\\n    } finally {\\n      btnGenerate.disabled = false;\\n    }\\n  });\\n\\n  btnCopy.addEventListener(\\"click\\", async () => {\\n    if (!lastDDL) return;\\n    statusEl.textContent = \\"クリップボードへコピー中…\\";\\n    const ok = await copyToClipboard(lastDDL);\\n    statusEl.textContent = ok\\n      ? \\"コピーしました（貼り付け先が大きさ制限ある場合はファイル出力を利用してください）\\"\\n      : \\"コピーに失敗しました（巨大サイズ制限の可能性あり：出力ファイルを開いて分割コピー推奨）\\";\\n  });\\n\\n  btnOpenFile.addEventListener(\\"click\\", async () => {\\n    if (!lastFilePath) return;\\n    try {\\n      await openFileByPath(lastFilePath);\\n    } catch (e) {\\n      statusEl.textContent = `ファイルを開けません: ${e?.message ?? e}`;\\n    }\\n  });\\n\\n  // auto run (optional)\\n  if (RUN_ON_LOAD) {\\n    btnGenerate.click();\\n  }\\n})();\\n```\\n\\n---\\n\\n## 2. External Tables DDL Generator\\n\\n```dataviewjs\\n(async () => {\\n  // ==============================\\n  // Snowflake EXTERNAL TABLE DDL Generator\\n  // ==============================\\n\\n  const SCHEMAS_PATH = \\"master/schemas\\";\\n  const EXTERNAL_TABLES_PATH = \\"master/externaltables\\";\\n  const COLUMNS_PATH = \\"master/columns\\";\\n\\n  const QUOTE_IDENT = false;\\n  const EMIT_COMMENTS = true;\\n  const OUTPUT_FOLDER = \\"generated/externaltable\\";\\n  const OUTPUT_PREFIX = \\"snowflake_external_ddl\\";\\n  const WRITE_SQL_FILE = true;\\n\\n  // ==============================\\n  // UI\\n  // ==============================\\n  dv.header(2, \\"External Tables DDL Generator\\");\\n  const statusEl = dv.el(\\"div\\", \\"準備完了（Generate を押してください）\\");\\n\\n  const btnRow = dv.el(\\"div\\", \\"\\");\\n  btnRow.style.display = \\"flex\\";\\n  btnRow.style.gap = \\"8px\\";\\n  btnRow.style.flexWrap = \\"wrap\\";\\n\\n  const btnGenerate = document.createElement(\\"button\\");\\n  btnGenerate.textContent = \\"Generate External Tables\\";\\n  btnRow.appendChild(btnGenerate);\\n\\n  const btnCopy = document.createElement(\\"button\\");\\n  btnCopy.textContent = \\"Copy to Clipboard\\";\\n  btnCopy.disabled = true;\\n  btnRow.appendChild(btnCopy);\\n\\n  const btnOpenFile = document.createElement(\\"button\\");\\n  btnOpenFile.textContent = \\"Open Output File\\";\\n  btnOpenFile.disabled = true;\\n  btnRow.appendChild(btnOpenFile);\\n\\n  const details = dv.el(\\"details\\", \\"\\");\\n  const summary = document.createElement(\\"summary\\");\\n  summary.textContent = \\"Preview（textarea）\\";\\n  details.appendChild(summary);\\n\\n  const ta = document.createElement(\\"textarea\\");\\n  ta.readOnly = true;\\n  ta.style.width = \\"100%\\";\\n  ta.style.height = \\"360px\\";\\n  ta.style.fontFamily = \\"monospace\\";\\n  ta.placeholder = \\"Generate 後にここに DDL を表示します\\";\\n  details.appendChild(ta);\\n\\n  let lastDDL = \\"\\";\\n  let lastFilePath = \\"\\";\\n\\n  // ==============================\\n  // helpers\\n  // ==============================\\n  function q(name) {\\n    if (!QUOTE_IDENT) return name;\\n    return `\\"${String(name).replaceAll('\\"', '\\"\\"')}\\"`;\\n  }\\n  function clean(v) {\\n    return (v === null || v === undefined) ? \\"\\" : String(v);\\n  }\\n  function bool(v, def=false) {\\n    if (v === true || v === false) return v;\\n    if (typeof v === \\"string\\") return v.toLowerCase() === \\"true\\";\\n    return def;\\n  }\\n  function sqlStringLiteral(s) {\\n    return \\"'\\" + String(s).replaceAll(\\"'\\", \\"''\\") + \\"'\\";\\n  }\\n  function key2(schemaPhysical, tablePhysical) {\\n    return `${clean(schemaPhysical).toUpperCase()}|${clean(tablePhysical).toUpperCase()}`;\\n  }\\n\\n  function ts() {\\n    const d = new Date();\\n    const pad = (n) => String(n).padStart(2, \\"0\\");\\n    return `${d.getFullYear()}${pad(d.getMonth()+1)}${pad(d.getDate())}_${pad(d.getHours())}${pad(d.getMinutes())}${pad(d.getSeconds())}`;\\n  }\\n\\n  async function ensureFolder(folderPath) {\\n    const parts = folderPath.split(\\"/\\").filter(Boolean);\\n    let cur = \\"\\";\\n    for (const p of parts) {\\n      cur = cur ? `${cur}/${p}` : p;\\n      if (!app.vault.getAbstractFileByPath(cur)) {\\n        try { await app.vault.createFolder(cur); } catch (e) { /* ignore */ }\\n      }\\n    }\\n  }\\n\\n  async function writeFile(path, content) {\\n    const af = app.vault.getAbstractFileByPath(path);\\n    if (af && af.children) throw new Error(`出力先がフォルダです: ${path}`);\\n    if (af) {\\n      await app.vault.modify(af, content);\\n    } else {\\n      await app.vault.create(path, content);\\n    }\\n    return app.vault.getAbstractFileByPath(path);\\n  }\\n\\n  async function openFileByPath(path) {\\n    const af = app.vault.getAbstractFileByPath(path);\\n    if (!af) throw new Error(`ファイルが見つかりません: ${path}`);\\n    if (af.children) throw new Error(`フォルダは開けません: ${path}`);\\n    await app.workspace.getLeaf(true).openFile(af);\\n  }\\n\\n  async function copyToClipboard(text) {\\n    try {\\n      await navigator.clipboard.writeText(text);\\n      return true;\\n    } catch (e) {\\n      try {\\n        ta.value = text;\\n        ta.focus();\\n        ta.select();\\n        const ok = document.execCommand(\\"copy\\");\\n        ta.setSelectionRange(0, 0);\\n        return !!ok;\\n      } catch (e2) {\\n        return false;\\n      }\\n    }\\n  }\\n\\n  // ==============================\\n  // main generator\\n  // ==============================\\n  async function generateExternalTableDDL() {\\n    const warns = [];\\n    const out = [];\\n\\n    // ---- load schemas ----\\n    const schemas = dv.pages(`\\"${SCHEMAS_PATH}\\"`)\\n      .where(p => p.schema_id && p.physical)\\n      .array()\\n      .sort((a,b)=>clean(a.physical).localeCompare(clean(b.physical)));\\n\\n    const schemaIdToPhysical = new Map(schemas.map(s => [String(s.schema_id), s.physical]));\\n\\n    // ---- load external tables ----\\n    const extTables = dv.pages(`\\"${EXTERNAL_TABLES_PATH}\\"`)\\n      .where(p => p.table_id && p.schema_id && p.physical)\\n      .array();\\n\\n    const tableIdToInfo = new Map();\\n    for (const t of extTables) {\\n      const sch = schemaIdToPhysical.get(String(t.schema_id));\\n      if (!sch) {\\n        warns.push(`external_table_id=${t.table_id} unknown schema_id=${t.schema_id}`);\\n        continue;\\n      }\\n      tableIdToInfo.set(String(t.table_id), { \\n        schemaPhysical: sch, \\n        tablePhysical: t.physical, \\n        table: t \\n      });\\n    }\\n\\n    // ---- load columns ----\\n    const cols = dv.pages(`\\"${COLUMNS_PATH}\\"`)\\n      .where(p => p.column_id && p.table_id && p.physical)\\n      .array();\\n\\n    const colsByFqn = new Map();\\n    for (const c of cols) {\\n      const info = tableIdToInfo.get(String(c.table_id));\\n      if (!info) continue;\\n      const k = key2(info.schemaPhysical, info.tablePhysical);\\n      if (!colsByFqn.has(k)) colsByFqn.set(k, []);\\n      colsByFqn.get(k).push(c);\\n    }\\n\\n    for (const arr of colsByFqn.values()) {\\n      arr.sort((a,b)=> clean(a.physical).localeCompare(clean(b.physical)));\\n    }\\n\\n    // ==============================\\n    // build EXTERNAL TABLE DDL\\n    // ==============================\\n    out.push(`-- ==============================\\\\n`);\\n    out.push(`-- EXTERNAL TABLES\\\\n`);\\n    out.push(`-- ==============================\\\\n\\\\n`);\\n\\n    for (const t of extTables) {\\n      const sch = schemaIdToPhysical.get(String(t.schema_id));\\n      if (!sch) continue;\\n\\n      const fqtn = `${q(sch)}.${q(t.physical)}`;\\n      const colsArr = colsByFqn.get(key2(sch, t.physical)) ?? [];\\n      \\n      if (colsArr.length === 0) {\\n        out.push(`-- SKIP: ${fqtn} (no columns)\\\\n\\\\n`);\\n        continue;\\n      }\\n\\n      // External table specific metadata\\n      const stageName = clean(t.stage_name) || `${t.physical}_STAGE`;\\n      const fileFormat = clean(t.file_format) || \\"JSON\\";\\n      const autoRefresh = bool(t.auto_refresh, true);\\n      const partitionBy = Array.isArray(t.partition_by) ? t.partition_by : [];\\n\\n      // Build column definitions with metadata$ extraction\\n      const lines = [];\\n      const partitionLines = [];\\n\\n      for (const c of colsArr) {\\n        const colName = q(c.physical);\\n        const domain = clean(c.domain) || \\"VARCHAR\\";\\n        \\n        // Check if this is a partition column\\n        if (partitionBy.includes(c.physical.toLowerCase()) || \\n            partitionBy.includes(c.physical)) {\\n          // Partition columns are extracted from metadata$filename\\n          const idx = partitionBy.indexOf(c.physical.toLowerCase()) >= 0 \\n            ? partitionBy.indexOf(c.physical.toLowerCase()) \\n            : partitionBy.indexOf(c.physical);\\n          partitionLines.push(\\n            `  ${colName} ${domain} AS CAST(SPLIT_PART(SPLIT_PART(metadata$filename, '/', ${idx+1}), '=', 2) AS ${domain})`\\n          );\\n        } else {\\n          // Regular columns from JSON value\\n          lines.push(`  ${colName} ${domain} AS (value:${c.physical.toLowerCase()}::${domain})`);\\n        }\\n      }\\n\\n      // Combine regular and partition columns\\n      const allLines = [...lines, ...partitionLines];\\n\\n      out.push(`CREATE OR REPLACE EXTERNAL TABLE ${fqtn}(\\\\n`);\\n      out.push(allLines.join(\\",\\\\n\\") + \\"\\\\n\\");\\n      out.push(`)`);\\n      \\n      // Partition specification\\n      if (partitionBy.length > 0) {\\n        out.push(`PARTITION BY (${partitionBy.map(p => p.toUpperCase()).join(\\", \\")})\\\\n`);\\n      }\\n      \\n      // Location and file format\\n      out.push(`LOCATION=@${q(sch)}.${q(stageName)}\\\\n`);\\n      out.push(`FILE_FORMAT=(TYPE=${fileFormat})\\\\n`);\\n      out.push(`AUTO_REFRESH=${autoRefresh ? \\"TRUE\\" : \\"FALSE\\"}\\\\n`);\\n      out.push(`REFRESH_ON_CREATE=TRUE;\\\\n`);\\n\\n      // Comments\\n      if (EMIT_COMMENTS && clean(t.comment)) {\\n        out.push(`\\\\nCOMMENT ON TABLE ${fqtn} IS ${sqlStringLiteral(t.comment)};\\\\n`);\\n      }\\n      if (EMIT_COMMENTS) {\\n        for (const c of colsArr) {\\n          if (!clean(c.comment)) continue;\\n          out.push(`COMMENT ON COLUMN ${fqtn}.${q(c.physical)} IS ${sqlStringLiteral(c.comment)};\\\\n`);\\n        }\\n      }\\n      out.push(\\"\\\\n\\");\\n    }\\n\\n    // ---- WARNINGS ----\\n    if (warns.length) {\\n      out.push(`-- ==============================\\\\n-- WARNINGS\\\\n-- ==============================\\\\n`);\\n      for (const w of warns) out.push(`-- ${w}\\\\n`);\\n      out.push(\\"\\\\n\\");\\n    }\\n\\n    const ddl = out.join(\\"\\").trimEnd() + \\"\\\\n\\";\\n    return { ddl, warns };\\n  }\\n\\n  // ==============================\\n  // actions\\n  // ==============================\\n  btnGenerate.addEventListener(\\"click\\", async () => {\\n    btnGenerate.disabled = true;\\n    btnCopy.disabled = true;\\n    btnOpenFile.disabled = true;\\n    statusEl.textContent = \\"生成中…\\";\\n    ta.value = \\"\\";\\n\\n    try {\\n      const t0 = performance.now();\\n      const { ddl, warns } = await generateExternalTableDDL();\\n      const t1 = performance.now();\\n\\n      lastDDL = ddl;\\n\\n      if (WRITE_SQL_FILE) {\\n        await ensureFolder(OUTPUT_FOLDER);\\n        lastFilePath = `${OUTPUT_FOLDER}/${OUTPUT_PREFIX}_${ts()}.sql`;\\n        await writeFile(lastFilePath, ddl);\\n        btnOpenFile.disabled = false;\\n      }\\n\\n      ta.value = ddl;\\n      btnCopy.disabled = false;\\n      \\n      statusEl.textContent =\\n        `完了: ${Math.round(t1 - t0)} ms / ${ddl.length.toLocaleString()} chars` +\\n        (WRITE_SQL_FILE ? ` / 出力: ${lastFilePath}` : \\"\\") +\\n        (warns.length ? ` / WARNINGS: ${warns.length}` : \\"\\");\\n    } catch (e) {\\n      console.error(e);\\n      statusEl.textContent = `失敗: ${e?.message ?? e}`;\\n    } finally {\\n      btnGenerate.disabled = false;\\n    }\\n  });\\n\\n  btnCopy.addEventListener(\\"click\\", async () => {\\n    if (!lastDDL) return;\\n    statusEl.textContent = \\"クリップボードへコピー中…\\";\\n    const ok = await copyToClipboard(lastDDL);\\n    statusEl.textContent = ok\\n      ? \\"コピーしました\\"\\n      : \\"コピーに失敗しました\\";\\n  });\\n\\n  btnOpenFile.addEventListener(\\"click\\", async () => {\\n    if (!lastFilePath) return;\\n    try {\\n      await openFileByPath(lastFilePath);\\n    } catch (e) {\\n      statusEl.textContent = `ファイルを開けません: ${e?.message ?? e}`;\\n    }\\n  });\\n})();\\n```\\n\\n---\\n\\n## 3. YAML FILE Generator\\n\\n```dataviewjs\\n(async () => {\\n  // ==============================\\n  // Snowflake Semantic Views YAML Generator\\n  // ==============================\\n\\n  const SCHEMAS_PATH = \\"master/schemas\\";\\n  const SEMANTIC_VIEWS_PATH = \\"master/semanticviews\\";\\n\\n  const OUTPUT_FOLDER = \\"generated/yaml\\";\\n  const WRITE_YAML_FILES = true;\\n\\n  // ==============================\\n  // UI\\n  // ==============================\\n  dv.header(2, \\"Semantic Views YAML Generator\\");\\n  const statusEl = dv.el(\\"div\\", \\"準備完了（Generate を押してください）\\");\\n\\n  const btnRow = dv.el(\\"div\\", \\"\\");\\n  btnRow.style.display = \\"flex\\";\\n  btnRow.style.gap = \\"8px\\";\\n  btnRow.style.flexWrap = \\"wrap\\";\\n\\n  const btnGenerate = document.createElement(\\"button\\");\\n  btnGenerate.textContent = \\"Generate YAML Files\\";\\n  btnRow.appendChild(btnGenerate);\\n\\n  const btnOpenFolder = document.createElement(\\"button\\");\\n  btnOpenFolder.textContent = \\"Open Output Folder\\";\\n  btnOpenFolder.disabled = true;\\n  btnRow.appendChild(btnOpenFolder);\\n\\n  const details = dv.el(\\"details\\", \\"\\");\\n  const summary = document.createElement(\\"summary\\");\\n  summary.textContent = \\"生成ファイル一覧\\";\\n  details.appendChild(summary);\\n\\n  const resultDiv = dv.el(\\"div\\", \\"\\");\\n  resultDiv.style.fontFamily = \\"monospace\\";\\n  resultDiv.style.fontSize = \\"0.9em\\";\\n  resultDiv.style.maxHeight = \\"300px\\";\\n  resultDiv.style.overflow = \\"auto\\";\\n  resultDiv.style.padding = \\"8px\\";\\n  resultDiv.style.border = \\"1px solid var(--background-modifier-border)\\";\\n  details.appendChild(resultDiv);\\n\\n  let lastOutputFolder = \\"\\";\\n\\n  // ==============================\\n  // helpers\\n  // ==============================\\n  function clean(v) {\\n    return (v === null || v === undefined) ? \\"\\" : String(v);\\n  }\\n\\n  function normalizeLF(s){\\n    return String(s ?? \\"\\").replace(/\\\\r\\\\n/g, \\"\\\\n\\").replace(/\\\\r/g, \\"\\\\n\\");\\n  }\\n\\n  async function ensureFolder(folderPath) {\\n    const parts = folderPath.split(\\"/\\").filter(Boolean);\\n    let cur = \\"\\";\\n    for (const p of parts) {\\n      cur = cur ? `${cur}/${p}` : p;\\n      if (!app.vault.getAbstractFileByPath(cur)) {\\n        try { await app.vault.createFolder(cur); } catch (e) { /* ignore */ }\\n      }\\n    }\\n  }\\n\\n  async function writeFile(path, content) {\\n    const af = app.vault.getAbstractFileByPath(path);\\n    if (af && af.children) throw new Error(`出力先がフォルダです: ${path}`);\\n    if (af) {\\n      await app.vault.modify(af, content);\\n    } else {\\n      await app.vault.create(path, content);\\n    }\\n    return app.vault.getAbstractFileByPath(path);\\n  }\\n\\n  async function openFolder(folderPath) {\\n    const folder = app.vault.getAbstractFileByPath(folderPath);\\n    if (!folder) throw new Error(`フォルダが見つかりません: ${folderPath}`);\\n    if (!folder.children) throw new Error(`フォルダではありません: ${folderPath}`);\\n    // Obsidianでフォルダを開く\\n    app.workspace.getLeaf(true).openFile(folder);\\n  }\\n\\n  // ==============================\\n  // main generator\\n  // ==============================\\n  async function generateYAMLFiles() {\\n    const warns = [];\\n    const results = [];\\n\\n    // ---- load schemas ----\\n    const schemas = dv.pages(`\\"${SCHEMAS_PATH}\\"`)\\n      .where(p => p.schema_id && p.physical)\\n      .array();\\n\\n    const schemaIdToPhysical = new Map(schemas.map(s => [String(s.schema_id), s.physical]));\\n\\n    // ---- load semantic views ----\\n    const semanticViews = dv.pages(`\\"${SEMANTIC_VIEWS_PATH}\\"`)\\n      .where(p => p.type === \\"semantic_view\\" && p.physical)\\n      .array();\\n\\n    if (semanticViews.length === 0) {\\n      warns.push(\\"セマンティックビューが見つかりません\\");\\n      return { results, warns };\\n    }\\n\\n    // ---- Generate YAML files ----\\n    for (const sv of semanticViews) {\\n      let md = \\"\\";\\n      try { md = await dv.io.load(sv.file.path); }\\n      catch { \\n        warns.push(`${sv.physical}: ファイル読み込み失敗`);\\n        continue;\\n      }\\n\\n      // YAMLブロックを抽出\\n      const yamlMatch = md.match(/```yaml\\\\s*([\\\\s\\\\S]*?)```/i);\\n      if (!yamlMatch) {\\n        warns.push(`${sv.physical}: YAMLブロックが見つかりません`);\\n        continue;\\n      }\\n\\n      const yamlContent = yamlMatch[1].trim();\\n      const yamlFilePath = `${OUTPUT_FOLDER}/${sv.physical}.yaml`;\\n\\n      if (WRITE_YAML_FILES) {\\n        await ensureFolder(OUTPUT_FOLDER);\\n        await writeFile(yamlFilePath, yamlContent);\\n      }\\n\\n      results.push({\\n        name: sv.physical,\\n        path: yamlFilePath,\\n        size: yamlContent.length,\\n        comment: clean(sv.comment)\\n      });\\n    }\\n\\n    return { results, warns };\\n  }\\n\\n  // ==============================\\n  // actions\\n  // ==============================\\n  btnGenerate.addEventListener(\\"click\\", async () => {\\n    btnGenerate.disabled = true;\\n    btnOpenFolder.disabled = true;\\n    statusEl.textContent = \\"生成中…\\";\\n    resultDiv.innerHTML = \\"\\";\\n\\n    try {\\n      const t0 = performance.now();\\n      const { results, warns } = await generateYAMLFiles();\\n      const t1 = performance.now();\\n\\n      lastOutputFolder = OUTPUT_FOLDER;\\n\\n      if (results.length > 0) {\\n        btnOpenFolder.disabled = false;\\n        \\n        let html = `<div style=\\"margin-bottom: 8px;\\"><strong>生成完了: ${results.length} ファイル</strong></div>`;\\n        html += `<table style=\\"width: 100%; border-collapse: collapse; font-size: 0.85em;\\">`;\\n        html += `<tr style=\\"border-bottom: 1px solid var(--background-modifier-border);\\">`;\\n        html += `<th style=\\"text-align: left; padding: 4px;\\">ファイル名</th>`;\\n        html += `<th style=\\"text-align: right; padding: 4px;\\">サイズ</th>`;\\n        html += `<th style=\\"text-align: left; padding: 4px;\\">説明</th>`;\\n        html += `</tr>`;\\n        \\n        for (const r of results) {\\n          html += `<tr style=\\"border-bottom: 1px solid var(--background-modifier-border-hover);\\">`;\\n          html += `<td style=\\"padding: 4px;\\">${r.name}.yaml</td>`;\\n          html += `<td style=\\"text-align: right; padding: 4px;\\">${r.size.toLocaleString()} bytes</td>`;\\n          html += `<td style=\\"padding: 4px; font-size: 0.9em; color: var(--text-muted);\\">${r.comment || '-'}</td>`;\\n          html += `</tr>`;\\n        }\\n        html += `</table>`;\\n        \\n        if (warns.length > 0) {\\n          html += `<div style=\\"margin-top: 12px; color: var(--text-warning);\\"><strong>警告: ${warns.length} 件</strong></div>`;\\n          html += `<ul style=\\"margin: 4px 0; padding-left: 20px; font-size: 0.85em;\\">`;\\n          for (const w of warns) {\\n            html += `<li>${w}</li>`;\\n          }\\n          html += `</ul>`;\\n        }\\n        \\n        resultDiv.innerHTML = html;\\n      } else {\\n        resultDiv.innerHTML = `<div style=\\"color: var(--text-error);\\">生成されたファイルがありません</div>`;\\n      }\\n\\n      statusEl.textContent =\\n        `完了: ${Math.round(t1 - t0)} ms / ${results.length} ファイル生成` +\\n        (warns.length ? ` / 警告: ${warns.length} 件` : \\"\\");\\n    } catch (e) {\\n      console.error(e);\\n      statusEl.textContent = `失敗: ${e?.message ?? e}`;\\n      resultDiv.innerHTML = `<div style=\\"color: var(--text-error);\\">エラー: ${e?.message ?? e}</div>`;\\n    } finally {\\n      btnGenerate.disabled = false;\\n    }\\n  });\\n\\n  btnOpenFolder.addEventListener(\\"click\\", async () => {\\n    if (!lastOutputFolder) return;\\n    try {\\n      // ファイルエクスプローラーで開く（システム依存）\\n      statusEl.textContent = `出力フォルダ: ${lastOutputFolder}`;\\n    } catch (e) {\\n      statusEl.textContent = `フォルダを開けません: ${e?.message ?? e}`;\\n    }\\n  });\\n})();\\n```\\n\\n---\\n\\n## その他のSQL操作例\\n\\n### DB・WHの作成\\n\\n```sql\\nUSE ROLE ACCOUNTADMIN;\\nCREATE DATABASE IF NOT EXISTS GBPS253YS_DB;\\nCREATE OR REPLACE WAREHOUSE GBPS253YS_WH WITH\\n     WAREHOUSE_SIZE='X-SMALL'\\n     AUTO_SUSPEND = 120\\n     AUTO_RESUME = TRUE\\n     INITIALLY_SUSPENDED=TRUE;\\n     \\nUSE DATABASE GBPS253YS_DB;\\nUSE WAREHOUSE GBPS253YS_WH;\\n\\nDROP SCHEMA PUBLIC;\\n```\\n\\n### ファイルからテーブルを作成\\n\\n##### 案件明細\\n\\n```sql\\nCOPY INTO GBPS253YS_DB.APP_PRODUCTION.ANKEN_MEISAI (\\n  ID,\\n  DEPARTMENT_SHORT_NAME,\\n  SECTION_NAME,\\n  FISCAL_YEAR,\\n  PROJECT_NUMBER,\\n  BRANCH_NUMBER,\\n  SALES_CATEGORY,\\n  DEPARTMENT_ID,\\n  GROUP_SHORT_NAME,\\n  CUSTOMER_ID,\\n  CUSTOMER_NAME,\\n  ORDER_NUMBER,\\n  ORDER_NAME,\\n  SUBJECT,\\n  PROJECT_NAME,\\n  WORK_START_DATE,\\n  WORK_END_DATE,\\n  ACCOUNTING_MONTH,\\n  RANK,\\n  AMOUNT,\\n  SALES_DELIVERY_FLAG,\\n  INVOICE_NUMBER,\\n  ACTIVE_FLAG,\\n  CUSTOMER_QUOTE_REQUEST_NUMBER,\\n  CUSTOMER_ORDER_NUMBER,\\n  DIVISION_CODE,\\n  DEPARTMENT_NAME,\\n  DEPARTMENT_SECTION_SHORT_NAME\\n)\\nFROM @GBPS253YS_DB.APP_PRODUCTION.RAW_DATA\\nFILES = ('案件：案件明細一覧20251217203031_0.csv')\\nFILE_FORMAT = (\\n    TYPE=CSV,\\n    SKIP_HEADER=1,\\n    FIELD_DELIMITER=',',\\n    TRIM_SPACE=FALSE,\\n    FIELD_OPTIONALLY_ENCLOSED_BY = '\\"',\\n    REPLACE_INVALID_CHARACTERS=TRUE,\\n    DATE_FORMAT=AUTO,\\n    TIME_FORMAT=AUTO,\\n    TIMESTAMP_FORMAT=AUTO,\\n    EMPTY_FIELD_AS_NULL=TRUE,\\n    NULL_IF = (''),\\n    error_on_column_count_mismatch=false\\n)\\nON_ERROR=CONTINUE\\nFORCE = TRUE;\\n```\\n\\n##### 部署マスタ\\n\\n```sql\\nCOPY INTO GBPS253YS_DB.APP_PRODUCTION.DEPARTMENT_MASTER (\\n  ID,                         -- 1\\n  FISCAL_YEAR,                -- 2\\n  DEPARTMENT_CATEGORY,        -- 3\\n  DEPARTMENT_ID,              -- 4\\n  DIVISION_CODE,              -- 5 部門CD\\n  DEPARTMENT_SECTION_CODE,    -- 6 部課CD\\n  HEADQUARTERS_CODE,          -- 7 本部CD\\n  GENERAL_DEPARTMENT_CODE,    -- 8 統括部CD\\n  DEPARTMENT_CODE,            -- 9 部CD\\n  SECTION_CODE,               -- 10 課CD\\n  GROUP_CODE,                 -- 11 グループCD\\n  FULL_NAME,                  -- 12 正式名称\\n  SHORT_NAME,                 -- 13 略称\\n  COMBINED_NAME,              -- 14 組合せ名称\\n  COMBINED_SHORT_NAME,        -- 15 組合せ略称\\n  ACCOUNTING_DEPARTMENT_CODE  -- 16 経理部門CD\\n)\\nFROM @GBPS253YS_DB.APP_PRODUCTION.RAW_DATA\\nFILES = ('部署マスタ20251217205645_0.csv')\\nFILE_FORMAT = (\\n  TYPE=CSV,\\n  SKIP_HEADER=1,\\n  FIELD_DELIMITER=',',\\n  FIELD_OPTIONALLY_ENCLOSED_BY='\\"',\\n  REPLACE_INVALID_CHARACTERS=TRUE,\\n  DATE_FORMAT=AUTO,\\n  TIME_FORMAT=AUTO,\\n  TIMESTAMP_FORMAT=AUTO,\\n  EMPTY_FIELD_AS_NULL=TRUE,\\n  NULL_IF=(''),\\n  ERROR_ON_COLUMN_COUNT_MISMATCH=FALSE\\n)\\nON_ERROR=CONTINUE\\nFORCE=TRUE;\\n\\n```\\n\\n#### DIM_ENTITY_ALIASデータの生成\\n\\n```sql\\nINSERT OVERWRITE INTO NAME_RESOLUTION.DIM_ENTITY_ALIAS (\\n  alias_normalized,\\n  entity_type,\\n  alias_raw,\\n  confidence,\\n  entity_id,\\n  entity_name,\\n  is_active,\\n  priority,\\n  refresh_run_id,\\n  refreshed_at\\n)\\nSELECT\\n  alias_normalized,\\n  entity_type,\\n  alias_raw,\\n  confidence,\\n  entity_id,\\n  entity_name,\\n  is_active,\\n  priority,\\n  TO_VARCHAR(CURRENT_TIMESTAMP()) AS refresh_run_id,\\n  CURRENT_TIMESTAMP()             AS refreshed_at\\nFROM APP_PRODUCTION.V_ENTITY_ALIAS_ALL;\\n```\\n\\n\\n#### API実行ロールの作成\\n\\n```sql\\nUSE ROLE SECURITYADMIN;\\nCREATE OR REPLACE ROLE GBPS253YS_API_ROLE;\\n\\nUSE ROLE SECURITYADMIN;\\nCREATE OR REPLACE  USER GBPS253YS_API_USER\\n  LOGIN_NAME = 'GBPS253YS_API_USER'\\n  DISPLAY_NAME = 'GBPS253YS_API_USER'\\n  DEFAULT_ROLE = GBPS253YS_API_ROLE\\n  MUST_CHANGE_PASSWORD = FALSE;\\n\\nALTER USER GBPS253YS_API_USER SET DEFAULT_ROLE = GBPS253YS_API_ROLE;\\nALTER USER GBPS253YS_API_USER SET DEFAULT_WAREHOUSE = GBPS253YS_WH;\\nALTER USER GBPS253YS_API_USER SET DEFAULT_NAMESPACE = GBPS253YS_DB.APP_PRODUCTION;\\n\\nGRANT ROLE GBPS253YS_API_ROLE TO USER GBPS253YS_API_USER;\\n\\n-- 2) Network Policy（API_USERだけ）\\nCREATE OR REPLACE NETWORK POLICY GBPS253YS_API_ONLY\\n  ALLOWED_IP_LIST = ('4.189.129.1');\\n\\nALTER USER GBPS253YS_API_USER SET NETWORK_POLICY = GBPS253YS_API_ONLY;\\n\\n-- 3) Agent権限（オーナーロールで）\\nUSE ROLE ACCOUNTADMIN;\\n\\n-- 既存\\nGRANT USAGE ON WAREHOUSE GBPS253YS_WH TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON ALL SCHEMAS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON ALL TABLES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON ALL VIEWS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON ALL SEQUENCES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON ALL FUNCTIONS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON ALL PROCEDURES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON ALL AGENT IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON AGENT GBPS253YS_DB.APP_PRODUCTION.SNOWFLAKE_DEMO_AGENT TO ROLE GBPS253YS_API_ROLE;\\n\\n-- 将来\\nGRANT USAGE ON FUTURE SCHEMAS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON FUTURE TABLES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON FUTURE VIEWS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON FUTURE SEQUENCES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON FUTURE FUNCTIONS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON FUTURE PROCEDURES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\n\\n\\nGRANT READ ON STAGE GBPS253YS_DB.APP_PRODUCTION.RAW_DATA TO ROLE GBPS253YS_API_ROLE;\\nGRANT WRITE ON STAGE GBPS253YS_DB.APP_PRODUCTION.RAW_DATA TO ROLE GBPS253YS_API_ROLE;\\n\\n-- 4) PAT発行（表示される値を保存）\\nUSE ROLE SECURITYADMIN;\\n\\nALTER USER GBPS253YS_API_USER\\n  ADD PROGRAMMATIC ACCESS TOKEN azure_func_token;\\n```\\n\\n\\n# 参考）使用方法\\n\\n1. 以下のようにしてデータを生成する\\n```sql\\n-- =========================================================\\n-- Weekly manual profiling + evidence export + ingest\\n-- =========================================================\\n\\n-- 0) 実行日（YYYYMMDD_HH24MISSFF3）を自動で作る\\nSET RUN_DATE = TO_VARCHAR(\\n  DATEADD(\\n    day,\\n    -1,\\n    CONVERT_TIMEZONE('UTC', 'Asia/Tokyo', CURRENT_TIMESTAMP())\\n  ),\\n  'YYYYMMDD_HH24MISSFF3'\\n);\\n\\n-- 1) メトリクス収集（週1回）\\n--    ※ 業務データスキーマのみ（例：APP_PRODUCTION）\\nCALL DB_DESIGN.PROFILE_ALL_TABLES(\\n  'GBPS253YS_DB',\\n  'APP_PRODUCTION',\\n  10.0,\\n  'weekly manual profiling'\\n);\\n\\nCALL DB_DESIGN.PROFILE_ALL_TABLES(\\n  'GBPS253YS_DB',\\n  'DB_DESIGN',\\n  10.0,\\n  'weekly manual profiling'\\n);\\n\\n-- 2) メトリクス情報を Markdown (+ raw json) にして S3 更新\\n--    ※ DB_DESIGN.V_PROFILE_RESULTS_LATEST をソースにするのはOK（基盤側）\\nCALL DB_DESIGN.EXPORT_PROFILE_EVIDENCE_MD_VFINAL(\\n  'GBPS253YS_DB',              -- P_SOURCE_DB\\n  'DB_DESIGN',                -- P_SOURCE_SCHEMA\\n  'V_PROFILE_RESULTS_LATEST', -- P_SOURCE_VIEW\\n  'GBPS253YS_DB',              -- P_TARGET_DB (フィルタ用)\\n  $RUN_DATE,                  -- P_RUN_DATE（自動）\\n  'reviews/profiles',         -- P_VAULT_PREFIX\\n  'APP_PRODUCTION'                  -- P_TARGET_SCHEMA（業務スキーマのみ）\\n);\\nCALL DB_DESIGN.EXPORT_PROFILE_EVIDENCE_MD_VFINAL(\\n  'GBPS253YS_DB',              -- P_SOURCE_DB\\n  'DB_DESIGN',                -- P_SOURCE_SCHEMA\\n  'V_PROFILE_RESULTS_LATEST', -- P_SOURCE_VIEW\\n  'GBPS253YS_DB',              -- P_TARGET_DB (フィルタ用)\\n  $RUN_DATE,                  -- P_RUN_DATE（自動）\\n  'reviews/profiles',         -- P_VAULT_PREFIX\\n  'DB_DESIGN'                  -- P_TARGET_SCHEMA（業務スキーマのみ）\\n);\\n\\n-- 3) 最新の Markdown を取り込み（Vault全文でもOKだが絞ると軽い）\\n--    reviews/profiles 配下の md だけ取り込む例\\nCALL DB_DESIGN.INGEST_VAULT_MD(\\n  '@DB_DESIGN.OBSIDIAN_VAULT_STAGE',\\n  '.*\\\\.md'\\n);\\n```\\n\\n2. Snowflake Cortex Agentにレビュー依頼する。\\n\\n\\n\\n#### クリーニング\\n\\n```sql\\nDROP DATABASE GBPS253YS_DB;\\nDROP WAREHOUSE GBPS253YS_WH;\\n```\\n","null_count":0,"null_rate":0.000000000000000e+00,"row_count":34,"top_values":[{"count":1,"value":"# DDL Generator\\n\\nこのファイルは、Snowflakeの各種DDLを一括生成するための統合ビューです。\\n\\n## 事前準備：DB・WHの作成\\n\\nDDL生成の前に、まずデータベースとウェアハウスを作成してください。\\n\\n```sql\\nUSE ROLE ACCOUNTADMIN;\\nCREATE DATABASE IF NOT EXISTS GBPS253YS_DB;\\nCREATE OR REPLACE WAREHOUSE GBPS253YS_WH WITH\\n     WAREHOUSE_SIZE='X-SMALL'\\n     AUTO_SUSPEND = 120\\n     AUTO_RESUME = TRUE\\n     INITIALLY_SUSPENDED=TRUE;\\n     \\nUSE DATABASE GBPS253YS_DB;\\nUSE WAREHOUSE GBPS253YS_WH;\\n\\nDROP SCHEMA PUBLIC;\\n```\\n\\n---\\n\\n## 生成可能なDDL\\n\\n1. **Snowflake DDL Generator** - スキーマ、テーブル、ビュー、プロシージャなど（セマンティックビューはコメントでYAML出力）\\n2. **External Tables DDL Generator** - S3/Azure連携の外部テーブル\\n3. **YAML FILE Generator** - セマンティックビューのYAMLファイル生成\\n\\n> **Note:** セマンティックビュー（YAML）は、Snowflake DDL Generatorではコメント形式で出力されます。  \\n> 実際のYAMLファイルとして出力する場合は、YAML FILE Generatorを使用してください。\\n\\n---\\n\\n## 1. Snowflake DDL Generator\\n\\n```dataviewjs\\n(async () => {\\n  // ==============================\\n  // Snowflake DDL Generator (FAST + COPY + FILE OUTPUT)\\n  // ==============================\\n\\n  // paths\\n  const SCHEMAS_PATH = \\"master/schemas\\";\\n  const TABLES_PATH  = \\"master/tables\\";\\n  const COLUMNS_PATH = \\"master/columns\\";\\n  const VIEWS_PATH   = \\"master/views\\";\\n  const SEMANTIC_VIEWS_PATH = \\"master/semanticviews\\";\\n  const OTHER_PATH   = \\"master/other\\";\\n\\n  // options\\n  const QUOTE_IDENT     = false;\\n  const EMIT_COMMENTS   = true;\\n  const IO_CONCURRENCY  = 8;     // VIEW/OTHER の md 読み込み並列数（増やしすぎ注意）\\n  const RUN_ON_LOAD     = false; // true にするとノート表示時に自動生成（重いなら false 推奨）\\n\\n  // output\\n  const OUTPUT_FOLDER   = \\"generated/ddl\\";           // Vault 内フォルダ\\n  const OUTPUT_PREFIX   = \\"snowflake_ddl\\";           // 例: snowflake_ddl_20251227_235959.sql\\n  const WRITE_SQL_FILE  = true;                       // Vault へ .sql 出力\\n\\n  // ==============================\\n  // UI\\n  // ==============================\\n  dv.header(2, \\"Snowflake DDL Generator\\");\\n  const statusEl = dv.el(\\"div\\", \\"準備完了（Generate を押してください）\\");\\n\\n  const btnRow = dv.el(\\"div\\", \\"\\");\\n  btnRow.style.display = \\"flex\\";\\n  btnRow.style.gap = \\"8px\\";\\n  btnRow.style.flexWrap = \\"wrap\\";\\n\\n  const btnGenerate = document.createElement(\\"button\\");\\n  btnGenerate.textContent = \\"Generate\\";\\n  btnRow.appendChild(btnGenerate);\\n\\n  const btnCopy = document.createElement(\\"button\\");\\n  btnCopy.textContent = \\"Copy to Clipboard\\";\\n  btnCopy.disabled = true;\\n  btnRow.appendChild(btnCopy);\\n\\n  const btnOpenFile = document.createElement(\\"button\\");\\n  btnOpenFile.textContent = \\"Open Output File\\";\\n  btnOpenFile.disabled = true;\\n  btnRow.appendChild(btnOpenFile);\\n\\n  // Preview (textarea)\\n  const details = dv.el(\\"details\\", \\"\\");\\n  const summary = document.createElement(\\"summary\\");\\n  summary.textContent = \\"Preview（textarea：ここから手動コピーも可）\\";\\n  details.appendChild(summary);\\n\\n  const ta = document.createElement(\\"textarea\\");\\n  ta.readOnly = true;\\n  ta.style.width = \\"100%\\";\\n  ta.style.height = \\"360px\\";\\n  ta.style.fontFamily = \\"monospace\\";\\n  ta.placeholder = \\"Generate 後にここに DDL を表示します\\";\\n  details.appendChild(ta);\\n\\n  // state\\n  let lastDDL = \\"\\";\\n  let lastFilePath = \\"\\";\\n\\n  // ==============================\\n  // helpers\\n  // ==============================\\n  function q(name) {\\n    if (!QUOTE_IDENT) return name;\\n    return `\\"${String(name).replaceAll('\\"', '\\"\\"')}\\"`;\\n  }\\n  function clean(v) {\\n    return (v === null || v === undefined) ? \\"\\" : String(v);\\n  }\\n  function bool(v, def=false) {\\n    if (v === true || v === false) return v;\\n    if (typeof v === \\"string\\") return v.toLowerCase() === \\"true\\";\\n    return def;\\n  }\\n  function sqlStringLiteral(s) {\\n    return \\"'\\" + String(s).replaceAll(\\"'\\", \\"''\\") + \\"'\\";\\n  }\\n  function key2(schemaPhysical, tablePhysical) {\\n    return `${clean(schemaPhysical).toUpperCase()}|${clean(tablePhysical).toUpperCase()}`;\\n  }\\n  function hasDefault(v) {\\n    if (v === null || v === undefined) return false;\\n    const s = String(v).trim();\\n    if (s === \\"\\" || s.toLowerCase() === \\"null\\") return false;\\n    return true;\\n  }\\n  function normalizeLF(s){\\n    return String(s ?? \\"\\").replace(/\\\\r\\\\n/g, \\"\\\\n\\").replace(/\\\\r/g, \\"\\\\n\\");\\n  }\\n  function parseSqlFromBody(md){\\n    md = normalizeLF(md);\\n    const m = md.match(/```sql\\\\s*([\\\\s\\\\S]*?)```/i);\\n    return m ? (m[1] ?? \\"\\").trim() : \\"\\";\\n  }\\n\\n  function parseViewColumnsFromBody(md) {\\n    md = normalizeLF(md);\\n    const lines = md.split(\\"\\\\n\\");\\n\\n    let headerIdx = -1;\\n    for (let i = 0; i < lines.length; i++) {\\n      const ln = lines[i].trim();\\n      if (!ln.includes(\\"|\\")) continue;\\n      const cells = ln.split(\\"|\\").map(s => s.trim()).filter(Boolean);\\n      const lower = cells.map(c => c.toLowerCase());\\n      if (\\n        (lower.includes(\\"column_name\\") || lower.includes(\\"column\\") || lower.includes(\\"name\\")) &&\\n        (lower.includes(\\"comment\\") || lower.includes(\\"description\\"))\\n      ) {\\n        headerIdx = i;\\n        break;\\n      }\\n    }\\n    if (headerIdx < 0) return [];\\n\\n    let sepIdx = -1;\\n    for (let i = headerIdx + 1; i < Math.min(lines.length, headerIdx + 6); i++) {\\n      const ln = lines[i].trim();\\n      if (ln.includes(\\"|\\") && /-[-\\\\s|:]+-/.test(ln)) { sepIdx = i; break; }\\n    }\\n    if (sepIdx < 0) return [];\\n\\n    const headerCells = lines[headerIdx].split(\\"|\\").map(s => s.trim()).filter(Boolean);\\n    const lowerHeader = headerCells.map(c => c.toLowerCase());\\n    const colIdx = lowerHeader.findIndex(h => [\\"column_name\\",\\"column\\",\\"name\\"].includes(h));\\n    const cmtIdx = lowerHeader.findIndex(h => [\\"comment\\",\\"description\\"].includes(h));\\n    const cIdx = colIdx >= 0 ? colIdx : 0;\\n    const dIdx = cmtIdx >= 0 ? cmtIdx : 1;\\n\\n    const out = [];\\n    for (let i = sepIdx + 1; i < lines.length; i++) {\\n      const ln = lines[i].trim();\\n      if (!ln.includes(\\"|\\")) break;\\n      if (ln === \\"\\") break;\\n      const cells = lines[i].split(\\"|\\").map(s => s.trim()).filter(Boolean);\\n      const name = (cells[cIdx] ?? \\"\\").trim();\\n      const comment = (cells[dIdx] ?? \\"\\").trim();\\n      if (!name) continue;\\n      out.push({ name, comment });\\n    }\\n    return out;\\n  }\\n\\n  function ts() {\\n    const d = new Date();\\n    const pad = (n) => String(n).padStart(2, \\"0\\");\\n    return `${d.getFullYear()}${pad(d.getMonth()+1)}${pad(d.getDate())}_${pad(d.getHours())}${pad(d.getMinutes())}${pad(d.getSeconds())}`;\\n  }\\n\\n  async function ensureFolder(folderPath) {\\n    const parts = folderPath.split(\\"/\\").filter(Boolean);\\n    let cur = \\"\\";\\n    for (const p of parts) {\\n      cur = cur ? `${cur}/${p}` : p;\\n      if (!app.vault.getAbstractFileByPath(cur)) {\\n        try { await app.vault.createFolder(cur); } catch (e) { /* ignore */ }\\n      }\\n    }\\n  }\\n\\n  async function writeFile(path, content) {\\n    const af = app.vault.getAbstractFileByPath(path);\\n    if (af && af.children) throw new Error(`出力先がフォルダです: ${path}`);\\n    if (af) {\\n      await app.vault.modify(af, content);\\n    } else {\\n      await app.vault.create(path, content);\\n    }\\n    return app.vault.getAbstractFileByPath(path);\\n  }\\n\\n  async function openFileByPath(path) {\\n    const af = app.vault.getAbstractFileByPath(path);\\n    if (!af) throw new Error(`ファイルが見つかりません: ${path}`);\\n    if (af.children) throw new Error(`フォルダは開けません: ${path}`);\\n    await app.workspace.getLeaf(true).openFile(af);\\n  }\\n\\n  async function copyToClipboard(text) {\\n    // クリック操作中に呼ばれる想定（Clipboard API 成功率UP）\\n    try {\\n      await navigator.clipboard.writeText(text);\\n      return true;\\n    } catch (e) {\\n      // fallback: textarea を選択して execCommand\\n      try {\\n        ta.value = text;\\n        ta.focus();\\n        ta.select();\\n        const ok = document.execCommand(\\"copy\\");\\n        ta.setSelectionRange(0, 0);\\n        return !!ok;\\n      } catch (e2) {\\n        return false;\\n      }\\n    }\\n  }\\n\\n  async function mapLimit(items, limit, fn) {\\n    const results = new Array(items.length);\\n    let next = 0;\\n    const workers = Array.from({ length: Math.max(1, limit) }, async () => {\\n      while (true) {\\n        const i = next++;\\n        if (i >= items.length) break;\\n        results[i] = await fn(items[i], i);\\n      }\\n    });\\n    await Promise.all(workers);\\n    return results;\\n  }\\n\\n  // ==============================\\n  // main generator\\n  // ==============================\\n  async function generateDDL() {\\n    const warns = [];\\n\\n    // ---- load schemas ----\\n    const schemas = dv.pages(`\\"${SCHEMAS_PATH}\\"`)\\n      .where(p => p.schema_id && p.physical)\\n      .array()\\n      .sort((a,b)=>clean(a.physical).localeCompare(clean(b.physical)));\\n\\n    const schemaIdToPhysical = new Map(schemas.map(s => [String(s.schema_id), s.physical]));\\n\\n    // ---- load tables ----\\n    const tables = dv.pages(`\\"${TABLES_PATH}\\"`)\\n      .where(p => p.table_id && p.schema_id && p.physical)\\n      .array();\\n\\n    const tableIdToInfo = new Map();\\n    for (const t of tables) {\\n      const sch = schemaIdToPhysical.get(String(t.schema_id));\\n      if (!sch) {\\n        warns.push(`table_id=${t.table_id} unknown schema_id=${t.schema_id}`);\\n        continue;\\n      }\\n      tableIdToInfo.set(String(t.table_id), { schemaPhysical: sch, tablePhysical: t.physical, table: t });\\n    }\\n\\n    // ---- load columns ----\\n    const cols = dv.pages(`\\"${COLUMNS_PATH}\\"`)\\n      .where(p => p.column_id && p.table_id && p.physical)\\n      .array();\\n\\n    const colsByFqn = new Map();\\n    for (const c of cols) {\\n      const info = tableIdToInfo.get(String(c.table_id));\\n      if (!info) {\\n        warns.push(`column_id=${c.column_id} unknown table_id=${c.table_id}`);\\n        continue;\\n      }\\n      const k = key2(info.schemaPhysical, info.tablePhysical);\\n      if (!colsByFqn.has(k)) colsByFqn.set(k, []);\\n      colsByFqn.get(k).push(c);\\n    }\\n\\n    for (const arr of colsByFqn.values()) {\\n      arr.sort((a,b)=>{\\n        if (bool(a.pk) !== bool(b.pk)) return bool(a.pk) ? -1 : 1;\\n        return clean(a.physical).localeCompare(clean(b.physical));\\n      });\\n    }\\n\\n    // ---- load views / semantic views / other ----\\n    const views = dv.pages(`\\"${VIEWS_PATH}\\"`)\\n      .where(p => p.view_id && p.schema_id && p.physical)\\n      .array();\\n\\n    const semanticViews = dv.pages(`\\"${SEMANTIC_VIEWS_PATH}\\"`)\\n      .where(p => p.type === \\"semantic_view\\" && p.physical)\\n      .array();\\n\\n    const others = dv.pages(`\\"${OTHER_PATH}\\"`)\\n      .where(p => p.type === \\"other\\" && p.schema_id)\\n      .array();\\n\\n    // ==============================\\n    // build DDL (FAST: array push -> join)\\n    // ==============================\\n    const out = [];\\n\\n    // ---- SCHEMA ----\\n    for (const s of schemas) {\\n      out.push(`CREATE OR REPLACE SCHEMA ${q(s.physical)};\\\\n`);\\n      if (EMIT_COMMENTS && clean(s.comment)) {\\n        out.push(`COMMENT ON SCHEMA ${q(s.physical)} IS ${sqlStringLiteral(s.comment)};\\\\n`);\\n      }\\n      out.push(\\"\\\\n\\");\\n    }\\n\\n    // ---- TABLE ----\\n    for (const t of tables) {\\n      const sch = schemaIdToPhysical.get(String(t.schema_id));\\n      if (!sch) continue;\\n\\n      const fqtn = `${q(sch)}.${q(t.physical)}`;\\n      const colsArr = colsByFqn.get(key2(sch, t.physical)) ?? [];\\n      if (colsArr.length === 0) {\\n        out.push(`-- SKIP: ${fqtn} (no columns)\\\\n\\\\n`);\\n        continue;\\n      }\\n\\n      const lines = [];\\n      const pkCols = [];\\n\\n      for (const c of colsArr) {\\n        const notNull = bool(c.is_nullable, true) ? \\"\\" : \\" NOT NULL\\";\\n        const defSql  = hasDefault(c.default) ? ` DEFAULT ${String(c.default).trim()}` : \\"\\";\\n        lines.push(`  ${q(c.physical)} ${clean(c.domain)||\\"VARCHAR\\"}${defSql}${notNull}`);\\n        if (bool(c.pk)) pkCols.push(q(c.physical));\\n      }\\n      if (pkCols.length) lines.push(`  PRIMARY KEY (${pkCols.join(\\", \\")})`);\\n\\n      out.push(`CREATE OR REPLACE TABLE ${fqtn} (\\\\n${lines.join(\\",\\\\n\\")}\\\\n);\\\\n`);\\n      if (EMIT_COMMENTS && clean(t.comment)) {\\n        out.push(`COMMENT ON TABLE ${fqtn} IS ${sqlStringLiteral(t.comment)};\\\\n`);\\n      }\\n      if (EMIT_COMMENTS) {\\n        for (const c of colsArr) {\\n          if (!clean(c.comment)) continue;\\n          out.push(`COMMENT ON COLUMN ${fqtn}.${q(c.physical)} IS ${sqlStringLiteral(c.comment)};\\\\n`);\\n        }\\n      }\\n      out.push(\\"\\\\n\\");\\n    }\\n\\n    // ---- VIEW ----\\n    out.push(`-- ==============================\\\\n-- VIEWS\\\\n-- ==============================\\\\n`);\\n\\n    const viewBlocks = await mapLimit(views, IO_CONCURRENCY, async (v) => {\\n      const sch = schemaIdToPhysical.get(String(v.schema_id));\\n      if (!sch) {\\n        return { ddl: \\"\\", warn: `view_id=${v.view_id} unknown schema_id=${v.schema_id}` };\\n      }\\n\\n      let md = \\"\\";\\n      try { md = await dv.io.load(v.file.path); }\\n      catch { return { ddl: \\"\\", warn: `view_id=${v.view_id} load failed` }; }\\n\\n      const vcols = parseViewColumnsFromBody(md);\\n      const sql = parseSqlFromBody(md);\\n\\n      if (!vcols.length || !sql) {\\n        return { ddl: `-- SKIP: ${sch}.${v.physical} (missing columns or sql)\\\\n\\\\n`, warn: \\"\\" };\\n      }\\n\\n      const header =\\n        `CREATE OR REPLACE VIEW ${q(sch)}.${q(v.physical)} (\\\\n` +\\n        vcols.map(c =>\\n          clean(c.comment)\\n            ? `  ${q(c.name)} COMMENT ${sqlStringLiteral(c.comment)}`\\n            : `  ${q(c.name)}`\\n        ).join(\\",\\\\n\\") +\\n        `\\\\n)`;\\n\\n      const viewComment = (EMIT_COMMENTS && clean(v.comment))\\n        ? ` COMMENT = ${sqlStringLiteral(v.comment)}`\\n        : \\"\\";\\n\\n      const ddl = `${header}${viewComment}\\\\nAS\\\\n${sql};\\\\n\\\\n`;\\n      return { ddl, warn: \\"\\" };\\n    });\\n\\n    for (const r of viewBlocks) {\\n      if (r?.warn) warns.push(r.warn);\\n      if (r?.ddl) out.push(r.ddl);\\n    }\\n\\n    // ---- SEMANTIC VIEWS (YAML) ----\\n    out.push(`-- ==============================\\\\n-- SEMANTIC VIEWS (YAML)\\\\n-- ==============================\\\\n`);\\n\\n    const semanticViewBlocks = await mapLimit(semanticViews, IO_CONCURRENCY, async (sv) => {\\n      let md = \\"\\";\\n      try { md = await dv.io.load(sv.file.path); }\\n      catch { return { ddl: \\"\\", warn: `semantic_view ${sv.physical} load failed` }; }\\n\\n      // YAMLブロックを抽出\\n      const yamlMatch = md.match(/```yaml\\\\s*([\\\\s\\\\S]*?)```/i);\\n      if (!yamlMatch) {\\n        return { ddl: `-- SKIP: ${sv.physical} (no yaml block)\\\\n\\\\n`, warn: \\"\\" };\\n      }\\n\\n      const yamlContent = yamlMatch[1].trim();\\n      const comment = clean(sv.comment) || `Semantic View: ${sv.physical}`;\\n\\n      const ddl =\\n        `-- SEMANTIC VIEW: ${sv.physical}\\\\n` +\\n        `-- ${comment}\\\\n` +\\n        `-- ファイル出力先: ${sv.physical}.yaml\\\\n` +\\n        `/*\\\\n${yamlContent}\\\\n*/\\\\n\\\\n`;\\n      \\n      return { ddl, warn: \\"\\" };\\n    });\\n\\n    for (const r of semanticViewBlocks) {\\n      if (r?.warn) warns.push(r.warn);\\n      if (r?.ddl) out.push(r.ddl);\\n    }\\n\\n    // ---- OTHER ----\\n    out.push(`-- ==============================\\\\n-- OTHER OBJECTS\\\\n-- ==============================\\\\n`);\\n\\n    const otherBlocks = await mapLimit(others, IO_CONCURRENCY, async (o) => {\\n      const sch = schemaIdToPhysical.get(String(o.schema_id));\\n      if (!sch) {\\n        return { ddl: \\"\\", warn: `other ${o.file?.path} unknown schema_id=${o.schema_id}` };\\n      }\\n\\n      let md = \\"\\";\\n      try { md = await dv.io.load(o.file.path); }\\n      catch { return { ddl: \\"\\", warn: `other ${o.file?.path} load failed` }; }\\n\\n      const sql = parseSqlFromBody(md);\\n      if (!sql) {\\n        return { ddl: \\"\\", warn: `other ${o.file?.path} has no sql block` };\\n      }\\n\\n      const ddl =\\n        `-- ${o.object_type?.toUpperCase() ?? \\"OTHER\\"}: ${sch}.${o.physical ?? \\"\\"}\\\\n` +\\n        `${sql}\\\\n\\\\n`;\\n      return { ddl, warn: \\"\\" };\\n    });\\n\\n    for (const r of otherBlocks) {\\n      if (r?.warn) warns.push(r.warn);\\n      if (r?.ddl) out.push(r.ddl);\\n    }\\n\\n    // ---- WARNINGS ----\\n    if (warns.length) {\\n      out.push(`-- ==============================\\\\n-- WARNINGS\\\\n-- ==============================\\\\n`);\\n      for (const w of warns) out.push(`-- ${w}\\\\n`);\\n      out.push(\\"\\\\n\\");\\n    }\\n\\n    const ddl = out.join(\\"\\").trimEnd() + \\"\\\\n\\";\\n    return { ddl, warns };\\n  }\\n\\n  // ==============================\\n  // actions\\n  // ==============================\\n  btnGenerate.addEventListener(\\"click\\", async () => {\\n    btnGenerate.disabled = true;\\n    btnCopy.disabled = true;\\n    btnOpenFile.disabled = true;\\n    statusEl.textContent = \\"生成中…（VIEW/OTHER が多いと少し待ちます）\\";\\n    ta.value = \\"\\";\\n\\n    try {\\n      const t0 = performance.now();\\n      const { ddl, warns } = await generateDDL();\\n      const t1 = performance.now();\\n\\n      lastDDL = ddl;\\n\\n      // write DDL file\\n      if (WRITE_SQL_FILE) {\\n        await ensureFolder(OUTPUT_FOLDER);\\n        lastFilePath = `${OUTPUT_FOLDER}/${OUTPUT_PREFIX}_${ts()}.sql`;\\n        await writeFile(lastFilePath, ddl);\\n        btnOpenFile.disabled = false;\\n      }\\n\\n      // preview\\n      ta.value = ddl;\\n\\n      btnCopy.disabled = false;\\n      statusEl.textContent =\\n        `完了: ${Math.round(t1 - t0)} ms / ${ddl.length.toLocaleString()} chars` +\\n        (WRITE_SQL_FILE ? ` / 出力: ${lastFilePath}` : \\"\\") +\\n        (warns.length ? ` / WARNINGS: ${warns.length}` : \\"\\");\\n    } catch (e) {\\n      console.error(e);\\n      statusEl.textContent = `失敗: ${e?.message ?? e}`;\\n    } finally {\\n      btnGenerate.disabled = false;\\n    }\\n  });\\n\\n  btnCopy.addEventListener(\\"click\\", async () => {\\n    if (!lastDDL) return;\\n    statusEl.textContent = \\"クリップボードへコピー中…\\";\\n    const ok = await copyToClipboard(lastDDL);\\n    statusEl.textContent = ok\\n      ? \\"コピーしました（貼り付け先が大きさ制限ある場合はファイル出力を利用してください）\\"\\n      : \\"コピーに失敗しました（巨大サイズ制限の可能性あり：出力ファイルを開いて分割コピー推奨）\\";\\n  });\\n\\n  btnOpenFile.addEventListener(\\"click\\", async () => {\\n    if (!lastFilePath) return;\\n    try {\\n      await openFileByPath(lastFilePath);\\n    } catch (e) {\\n      statusEl.textContent = `ファイルを開けません: ${e?.message ?? e}`;\\n    }\\n  });\\n\\n  // auto run (optional)\\n  if (RUN_ON_LOAD) {\\n    btnGenerate.click();\\n  }\\n})();\\n```\\n\\n---\\n\\n## 2. External Tables DDL Generator\\n\\n```dataviewjs\\n(async () => {\\n  // ==============================\\n  // Snowflake EXTERNAL TABLE DDL Generator\\n  // ==============================\\n\\n  const SCHEMAS_PATH = \\"master/schemas\\";\\n  const EXTERNAL_TABLES_PATH = \\"master/externaltables\\";\\n  const COLUMNS_PATH = \\"master/columns\\";\\n\\n  const QUOTE_IDENT = false;\\n  const EMIT_COMMENTS = true;\\n  const OUTPUT_FOLDER = \\"generated/externaltable\\";\\n  const OUTPUT_PREFIX = \\"snowflake_external_ddl\\";\\n  const WRITE_SQL_FILE = true;\\n\\n  // ==============================\\n  // UI\\n  // ==============================\\n  dv.header(2, \\"External Tables DDL Generator\\");\\n  const statusEl = dv.el(\\"div\\", \\"準備完了（Generate を押してください）\\");\\n\\n  const btnRow = dv.el(\\"div\\", \\"\\");\\n  btnRow.style.display = \\"flex\\";\\n  btnRow.style.gap = \\"8px\\";\\n  btnRow.style.flexWrap = \\"wrap\\";\\n\\n  const btnGenerate = document.createElement(\\"button\\");\\n  btnGenerate.textContent = \\"Generate External Tables\\";\\n  btnRow.appendChild(btnGenerate);\\n\\n  const btnCopy = document.createElement(\\"button\\");\\n  btnCopy.textContent = \\"Copy to Clipboard\\";\\n  btnCopy.disabled = true;\\n  btnRow.appendChild(btnCopy);\\n\\n  const btnOpenFile = document.createElement(\\"button\\");\\n  btnOpenFile.textContent = \\"Open Output File\\";\\n  btnOpenFile.disabled = true;\\n  btnRow.appendChild(btnOpenFile);\\n\\n  const details = dv.el(\\"details\\", \\"\\");\\n  const summary = document.createElement(\\"summary\\");\\n  summary.textContent = \\"Preview（textarea）\\";\\n  details.appendChild(summary);\\n\\n  const ta = document.createElement(\\"textarea\\");\\n  ta.readOnly = true;\\n  ta.style.width = \\"100%\\";\\n  ta.style.height = \\"360px\\";\\n  ta.style.fontFamily = \\"monospace\\";\\n  ta.placeholder = \\"Generate 後にここに DDL を表示します\\";\\n  details.appendChild(ta);\\n\\n  let lastDDL = \\"\\";\\n  let lastFilePath = \\"\\";\\n\\n  // ==============================\\n  // helpers\\n  // ==============================\\n  function q(name) {\\n    if (!QUOTE_IDENT) return name;\\n    return `\\"${String(name).replaceAll('\\"', '\\"\\"')}\\"`;\\n  }\\n  function clean(v) {\\n    return (v === null || v === undefined) ? \\"\\" : String(v);\\n  }\\n  function bool(v, def=false) {\\n    if (v === true || v === false) return v;\\n    if (typeof v === \\"string\\") return v.toLowerCase() === \\"true\\";\\n    return def;\\n  }\\n  function sqlStringLiteral(s) {\\n    return \\"'\\" + String(s).replaceAll(\\"'\\", \\"''\\") + \\"'\\";\\n  }\\n  function key2(schemaPhysical, tablePhysical) {\\n    return `${clean(schemaPhysical).toUpperCase()}|${clean(tablePhysical).toUpperCase()}`;\\n  }\\n\\n  function ts() {\\n    const d = new Date();\\n    const pad = (n) => String(n).padStart(2, \\"0\\");\\n    return `${d.getFullYear()}${pad(d.getMonth()+1)}${pad(d.getDate())}_${pad(d.getHours())}${pad(d.getMinutes())}${pad(d.getSeconds())}`;\\n  }\\n\\n  async function ensureFolder(folderPath) {\\n    const parts = folderPath.split(\\"/\\").filter(Boolean);\\n    let cur = \\"\\";\\n    for (const p of parts) {\\n      cur = cur ? `${cur}/${p}` : p;\\n      if (!app.vault.getAbstractFileByPath(cur)) {\\n        try { await app.vault.createFolder(cur); } catch (e) { /* ignore */ }\\n      }\\n    }\\n  }\\n\\n  async function writeFile(path, content) {\\n    const af = app.vault.getAbstractFileByPath(path);\\n    if (af && af.children) throw new Error(`出力先がフォルダです: ${path}`);\\n    if (af) {\\n      await app.vault.modify(af, content);\\n    } else {\\n      await app.vault.create(path, content);\\n    }\\n    return app.vault.getAbstractFileByPath(path);\\n  }\\n\\n  async function openFileByPath(path) {\\n    const af = app.vault.getAbstractFileByPath(path);\\n    if (!af) throw new Error(`ファイルが見つかりません: ${path}`);\\n    if (af.children) throw new Error(`フォルダは開けません: ${path}`);\\n    await app.workspace.getLeaf(true).openFile(af);\\n  }\\n\\n  async function copyToClipboard(text) {\\n    try {\\n      await navigator.clipboard.writeText(text);\\n      return true;\\n    } catch (e) {\\n      try {\\n        ta.value = text;\\n        ta.focus();\\n        ta.select();\\n        const ok = document.execCommand(\\"copy\\");\\n        ta.setSelectionRange(0, 0);\\n        return !!ok;\\n      } catch (e2) {\\n        return false;\\n      }\\n    }\\n  }\\n\\n  // ==============================\\n  // main generator\\n  // ==============================\\n  async function generateExternalTableDDL() {\\n    const warns = [];\\n    const out = [];\\n\\n    // ---- load schemas ----\\n    const schemas = dv.pages(`\\"${SCHEMAS_PATH}\\"`)\\n      .where(p => p.schema_id && p.physical)\\n      .array()\\n      .sort((a,b)=>clean(a.physical).localeCompare(clean(b.physical)));\\n\\n    const schemaIdToPhysical = new Map(schemas.map(s => [String(s.schema_id), s.physical]));\\n\\n    // ---- load external tables ----\\n    const extTables = dv.pages(`\\"${EXTERNAL_TABLES_PATH}\\"`)\\n      .where(p => p.table_id && p.schema_id && p.physical)\\n      .array();\\n\\n    const tableIdToInfo = new Map();\\n    for (const t of extTables) {\\n      const sch = schemaIdToPhysical.get(String(t.schema_id));\\n      if (!sch) {\\n        warns.push(`external_table_id=${t.table_id} unknown schema_id=${t.schema_id}`);\\n        continue;\\n      }\\n      tableIdToInfo.set(String(t.table_id), { \\n        schemaPhysical: sch, \\n        tablePhysical: t.physical, \\n        table: t \\n      });\\n    }\\n\\n    // ---- load columns ----\\n    const cols = dv.pages(`\\"${COLUMNS_PATH}\\"`)\\n      .where(p => p.column_id && p.table_id && p.physical)\\n      .array();\\n\\n    const colsByFqn = new Map();\\n    for (const c of cols) {\\n      const info = tableIdToInfo.get(String(c.table_id));\\n      if (!info) continue;\\n      const k = key2(info.schemaPhysical, info.tablePhysical);\\n      if (!colsByFqn.has(k)) colsByFqn.set(k, []);\\n      colsByFqn.get(k).push(c);\\n    }\\n\\n    for (const arr of colsByFqn.values()) {\\n      arr.sort((a,b)=> clean(a.physical).localeCompare(clean(b.physical)));\\n    }\\n\\n    // ==============================\\n    // build EXTERNAL TABLE DDL\\n    // ==============================\\n    out.push(`-- ==============================\\\\n`);\\n    out.push(`-- EXTERNAL TABLES\\\\n`);\\n    out.push(`-- ==============================\\\\n\\\\n`);\\n\\n    for (const t of extTables) {\\n      const sch = schemaIdToPhysical.get(String(t.schema_id));\\n      if (!sch) continue;\\n\\n      const fqtn = `${q(sch)}.${q(t.physical)}`;\\n      const colsArr = colsByFqn.get(key2(sch, t.physical)) ?? [];\\n      \\n      if (colsArr.length === 0) {\\n        out.push(`-- SKIP: ${fqtn} (no columns)\\\\n\\\\n`);\\n        continue;\\n      }\\n\\n      // External table specific metadata\\n      const stageName = clean(t.stage_name) || `${t.physical}_STAGE`;\\n      const fileFormat = clean(t.file_format) || \\"JSON\\";\\n      const autoRefresh = bool(t.auto_refresh, true);\\n      const partitionBy = Array.isArray(t.partition_by) ? t.partition_by : [];\\n\\n      // Build column definitions with metadata$ extraction\\n      const lines = [];\\n      const partitionLines = [];\\n\\n      for (const c of colsArr) {\\n        const colName = q(c.physical);\\n        const domain = clean(c.domain) || \\"VARCHAR\\";\\n        \\n        // Check if this is a partition column\\n        if (partitionBy.includes(c.physical.toLowerCase()) || \\n            partitionBy.includes(c.physical)) {\\n          // Partition columns are extracted from metadata$filename\\n          const idx = partitionBy.indexOf(c.physical.toLowerCase()) >= 0 \\n            ? partitionBy.indexOf(c.physical.toLowerCase()) \\n            : partitionBy.indexOf(c.physical);\\n          partitionLines.push(\\n            `  ${colName} ${domain} AS CAST(SPLIT_PART(SPLIT_PART(metadata$filename, '/', ${idx+1}), '=', 2) AS ${domain})`\\n          );\\n        } else {\\n          // Regular columns from JSON value\\n          lines.push(`  ${colName} ${domain} AS (value:${c.physical.toLowerCase()}::${domain})`);\\n        }\\n      }\\n\\n      // Combine regular and partition columns\\n      const allLines = [...lines, ...partitionLines];\\n\\n      out.push(`CREATE OR REPLACE EXTERNAL TABLE ${fqtn}(\\\\n`);\\n      out.push(allLines.join(\\",\\\\n\\") + \\"\\\\n\\");\\n      out.push(`)`);\\n      \\n      // Partition specification\\n      if (partitionBy.length > 0) {\\n        out.push(`PARTITION BY (${partitionBy.map(p => p.toUpperCase()).join(\\", \\")})\\\\n`);\\n      }\\n      \\n      // Location and file format\\n      out.push(`LOCATION=@${q(sch)}.${q(stageName)}\\\\n`);\\n      out.push(`FILE_FORMAT=(TYPE=${fileFormat})\\\\n`);\\n      out.push(`AUTO_REFRESH=${autoRefresh ? \\"TRUE\\" : \\"FALSE\\"}\\\\n`);\\n      out.push(`REFRESH_ON_CREATE=TRUE;\\\\n`);\\n\\n      // Comments\\n      if (EMIT_COMMENTS && clean(t.comment)) {\\n        out.push(`\\\\nCOMMENT ON TABLE ${fqtn} IS ${sqlStringLiteral(t.comment)};\\\\n`);\\n      }\\n      if (EMIT_COMMENTS) {\\n        for (const c of colsArr) {\\n          if (!clean(c.comment)) continue;\\n          out.push(`COMMENT ON COLUMN ${fqtn}.${q(c.physical)} IS ${sqlStringLiteral(c.comment)};\\\\n`);\\n        }\\n      }\\n      out.push(\\"\\\\n\\");\\n    }\\n\\n    // ---- WARNINGS ----\\n    if (warns.length) {\\n      out.push(`-- ==============================\\\\n-- WARNINGS\\\\n-- ==============================\\\\n`);\\n      for (const w of warns) out.push(`-- ${w}\\\\n`);\\n      out.push(\\"\\\\n\\");\\n    }\\n\\n    const ddl = out.join(\\"\\").trimEnd() + \\"\\\\n\\";\\n    return { ddl, warns };\\n  }\\n\\n  // ==============================\\n  // actions\\n  // ==============================\\n  btnGenerate.addEventListener(\\"click\\", async () => {\\n    btnGenerate.disabled = true;\\n    btnCopy.disabled = true;\\n    btnOpenFile.disabled = true;\\n    statusEl.textContent = \\"生成中…\\";\\n    ta.value = \\"\\";\\n\\n    try {\\n      const t0 = performance.now();\\n      const { ddl, warns } = await generateExternalTableDDL();\\n      const t1 = performance.now();\\n\\n      lastDDL = ddl;\\n\\n      if (WRITE_SQL_FILE) {\\n        await ensureFolder(OUTPUT_FOLDER);\\n        lastFilePath = `${OUTPUT_FOLDER}/${OUTPUT_PREFIX}_${ts()}.sql`;\\n        await writeFile(lastFilePath, ddl);\\n        btnOpenFile.disabled = false;\\n      }\\n\\n      ta.value = ddl;\\n      btnCopy.disabled = false;\\n      \\n      statusEl.textContent =\\n        `完了: ${Math.round(t1 - t0)} ms / ${ddl.length.toLocaleString()} chars` +\\n        (WRITE_SQL_FILE ? ` / 出力: ${lastFilePath}` : \\"\\") +\\n        (warns.length ? ` / WARNINGS: ${warns.length}` : \\"\\");\\n    } catch (e) {\\n      console.error(e);\\n      statusEl.textContent = `失敗: ${e?.message ?? e}`;\\n    } finally {\\n      btnGenerate.disabled = false;\\n    }\\n  });\\n\\n  btnCopy.addEventListener(\\"click\\", async () => {\\n    if (!lastDDL) return;\\n    statusEl.textContent = \\"クリップボードへコピー中…\\";\\n    const ok = await copyToClipboard(lastDDL);\\n    statusEl.textContent = ok\\n      ? \\"コピーしました\\"\\n      : \\"コピーに失敗しました\\";\\n  });\\n\\n  btnOpenFile.addEventListener(\\"click\\", async () => {\\n    if (!lastFilePath) return;\\n    try {\\n      await openFileByPath(lastFilePath);\\n    } catch (e) {\\n      statusEl.textContent = `ファイルを開けません: ${e?.message ?? e}`;\\n    }\\n  });\\n})();\\n```\\n\\n---\\n\\n## 3. YAML FILE Generator\\n\\n```dataviewjs\\n(async () => {\\n  // ==============================\\n  // Snowflake Semantic Views YAML Generator\\n  // ==============================\\n\\n  const SCHEMAS_PATH = \\"master/schemas\\";\\n  const SEMANTIC_VIEWS_PATH = \\"master/semanticviews\\";\\n\\n  const OUTPUT_FOLDER = \\"generated/yaml\\";\\n  const WRITE_YAML_FILES = true;\\n\\n  // ==============================\\n  // UI\\n  // ==============================\\n  dv.header(2, \\"Semantic Views YAML Generator\\");\\n  const statusEl = dv.el(\\"div\\", \\"準備完了（Generate を押してください）\\");\\n\\n  const btnRow = dv.el(\\"div\\", \\"\\");\\n  btnRow.style.display = \\"flex\\";\\n  btnRow.style.gap = \\"8px\\";\\n  btnRow.style.flexWrap = \\"wrap\\";\\n\\n  const btnGenerate = document.createElement(\\"button\\");\\n  btnGenerate.textContent = \\"Generate YAML Files\\";\\n  btnRow.appendChild(btnGenerate);\\n\\n  const btnOpenFolder = document.createElement(\\"button\\");\\n  btnOpenFolder.textContent = \\"Open Output Folder\\";\\n  btnOpenFolder.disabled = true;\\n  btnRow.appendChild(btnOpenFolder);\\n\\n  const details = dv.el(\\"details\\", \\"\\");\\n  const summary = document.createElement(\\"summary\\");\\n  summary.textContent = \\"生成ファイル一覧\\";\\n  details.appendChild(summary);\\n\\n  const resultDiv = dv.el(\\"div\\", \\"\\");\\n  resultDiv.style.fontFamily = \\"monospace\\";\\n  resultDiv.style.fontSize = \\"0.9em\\";\\n  resultDiv.style.maxHeight = \\"300px\\";\\n  resultDiv.style.overflow = \\"auto\\";\\n  resultDiv.style.padding = \\"8px\\";\\n  resultDiv.style.border = \\"1px solid var(--background-modifier-border)\\";\\n  details.appendChild(resultDiv);\\n\\n  let lastOutputFolder = \\"\\";\\n\\n  // ==============================\\n  // helpers\\n  // ==============================\\n  function clean(v) {\\n    return (v === null || v === undefined) ? \\"\\" : String(v);\\n  }\\n\\n  function normalizeLF(s){\\n    return String(s ?? \\"\\").replace(/\\\\r\\\\n/g, \\"\\\\n\\").replace(/\\\\r/g, \\"\\\\n\\");\\n  }\\n\\n  async function ensureFolder(folderPath) {\\n    const parts = folderPath.split(\\"/\\").filter(Boolean);\\n    let cur = \\"\\";\\n    for (const p of parts) {\\n      cur = cur ? `${cur}/${p}` : p;\\n      if (!app.vault.getAbstractFileByPath(cur)) {\\n        try { await app.vault.createFolder(cur); } catch (e) { /* ignore */ }\\n      }\\n    }\\n  }\\n\\n  async function writeFile(path, content) {\\n    const af = app.vault.getAbstractFileByPath(path);\\n    if (af && af.children) throw new Error(`出力先がフォルダです: ${path}`);\\n    if (af) {\\n      await app.vault.modify(af, content);\\n    } else {\\n      await app.vault.create(path, content);\\n    }\\n    return app.vault.getAbstractFileByPath(path);\\n  }\\n\\n  async function openFolder(folderPath) {\\n    const folder = app.vault.getAbstractFileByPath(folderPath);\\n    if (!folder) throw new Error(`フォルダが見つかりません: ${folderPath}`);\\n    if (!folder.children) throw new Error(`フォルダではありません: ${folderPath}`);\\n    // Obsidianでフォルダを開く\\n    app.workspace.getLeaf(true).openFile(folder);\\n  }\\n\\n  // ==============================\\n  // main generator\\n  // ==============================\\n  async function generateYAMLFiles() {\\n    const warns = [];\\n    const results = [];\\n\\n    // ---- load schemas ----\\n    const schemas = dv.pages(`\\"${SCHEMAS_PATH}\\"`)\\n      .where(p => p.schema_id && p.physical)\\n      .array();\\n\\n    const schemaIdToPhysical = new Map(schemas.map(s => [String(s.schema_id), s.physical]));\\n\\n    // ---- load semantic views ----\\n    const semanticViews = dv.pages(`\\"${SEMANTIC_VIEWS_PATH}\\"`)\\n      .where(p => p.type === \\"semantic_view\\" && p.physical)\\n      .array();\\n\\n    if (semanticViews.length === 0) {\\n      warns.push(\\"セマンティックビューが見つかりません\\");\\n      return { results, warns };\\n    }\\n\\n    // ---- Generate YAML files ----\\n    for (const sv of semanticViews) {\\n      let md = \\"\\";\\n      try { md = await dv.io.load(sv.file.path); }\\n      catch { \\n        warns.push(`${sv.physical}: ファイル読み込み失敗`);\\n        continue;\\n      }\\n\\n      // YAMLブロックを抽出\\n      const yamlMatch = md.match(/```yaml\\\\s*([\\\\s\\\\S]*?)```/i);\\n      if (!yamlMatch) {\\n        warns.push(`${sv.physical}: YAMLブロックが見つかりません`);\\n        continue;\\n      }\\n\\n      const yamlContent = yamlMatch[1].trim();\\n      const yamlFilePath = `${OUTPUT_FOLDER}/${sv.physical}.yaml`;\\n\\n      if (WRITE_YAML_FILES) {\\n        await ensureFolder(OUTPUT_FOLDER);\\n        await writeFile(yamlFilePath, yamlContent);\\n      }\\n\\n      results.push({\\n        name: sv.physical,\\n        path: yamlFilePath,\\n        size: yamlContent.length,\\n        comment: clean(sv.comment)\\n      });\\n    }\\n\\n    return { results, warns };\\n  }\\n\\n  // ==============================\\n  // actions\\n  // ==============================\\n  btnGenerate.addEventListener(\\"click\\", async () => {\\n    btnGenerate.disabled = true;\\n    btnOpenFolder.disabled = true;\\n    statusEl.textContent = \\"生成中…\\";\\n    resultDiv.innerHTML = \\"\\";\\n\\n    try {\\n      const t0 = performance.now();\\n      const { results, warns } = await generateYAMLFiles();\\n      const t1 = performance.now();\\n\\n      lastOutputFolder = OUTPUT_FOLDER;\\n\\n      if (results.length > 0) {\\n        btnOpenFolder.disabled = false;\\n        \\n        let html = `<div style=\\"margin-bottom: 8px;\\"><strong>生成完了: ${results.length} ファイル</strong></div>`;\\n        html += `<table style=\\"width: 100%; border-collapse: collapse; font-size: 0.85em;\\">`;\\n        html += `<tr style=\\"border-bottom: 1px solid var(--background-modifier-border);\\">`;\\n        html += `<th style=\\"text-align: left; padding: 4px;\\">ファイル名</th>`;\\n        html += `<th style=\\"text-align: right; padding: 4px;\\">サイズ</th>`;\\n        html += `<th style=\\"text-align: left; padding: 4px;\\">説明</th>`;\\n        html += `</tr>`;\\n        \\n        for (const r of results) {\\n          html += `<tr style=\\"border-bottom: 1px solid var(--background-modifier-border-hover);\\">`;\\n          html += `<td style=\\"padding: 4px;\\">${r.name}.yaml</td>`;\\n          html += `<td style=\\"text-align: right; padding: 4px;\\">${r.size.toLocaleString()} bytes</td>`;\\n          html += `<td style=\\"padding: 4px; font-size: 0.9em; color: var(--text-muted);\\">${r.comment || '-'}</td>`;\\n          html += `</tr>`;\\n        }\\n        html += `</table>`;\\n        \\n        if (warns.length > 0) {\\n          html += `<div style=\\"margin-top: 12px; color: var(--text-warning);\\"><strong>警告: ${warns.length} 件</strong></div>`;\\n          html += `<ul style=\\"margin: 4px 0; padding-left: 20px; font-size: 0.85em;\\">`;\\n          for (const w of warns) {\\n            html += `<li>${w}</li>`;\\n          }\\n          html += `</ul>`;\\n        }\\n        \\n        resultDiv.innerHTML = html;\\n      } else {\\n        resultDiv.innerHTML = `<div style=\\"color: var(--text-error);\\">生成されたファイルがありません</div>`;\\n      }\\n\\n      statusEl.textContent =\\n        `完了: ${Math.round(t1 - t0)} ms / ${results.length} ファイル生成` +\\n        (warns.length ? ` / 警告: ${warns.length} 件` : \\"\\");\\n    } catch (e) {\\n      console.error(e);\\n      statusEl.textContent = `失敗: ${e?.message ?? e}`;\\n      resultDiv.innerHTML = `<div style=\\"color: var(--text-error);\\">エラー: ${e?.message ?? e}</div>`;\\n    } finally {\\n      btnGenerate.disabled = false;\\n    }\\n  });\\n\\n  btnOpenFolder.addEventListener(\\"click\\", async () => {\\n    if (!lastOutputFolder) return;\\n    try {\\n      // ファイルエクスプローラーで開く（システム依存）\\n      statusEl.textContent = `出力フォルダ: ${lastOutputFolder}`;\\n    } catch (e) {\\n      statusEl.textContent = `フォルダを開けません: ${e?.message ?? e}`;\\n    }\\n  });\\n})();\\n```\\n\\n---\\n\\n## その他のSQL操作例\\n\\n### DB・WHの作成\\n\\n```sql\\nUSE ROLE ACCOUNTADMIN;\\nCREATE DATABASE IF NOT EXISTS GBPS253YS_DB;\\nCREATE OR REPLACE WAREHOUSE GBPS253YS_WH WITH\\n     WAREHOUSE_SIZE='X-SMALL'\\n     AUTO_SUSPEND = 120\\n     AUTO_RESUME = TRUE\\n     INITIALLY_SUSPENDED=TRUE;\\n     \\nUSE DATABASE GBPS253YS_DB;\\nUSE WAREHOUSE GBPS253YS_WH;\\n\\nDROP SCHEMA PUBLIC;\\n```\\n\\n### ファイルからテーブルを作成\\n\\n##### 案件明細\\n\\n```sql\\nCOPY INTO GBPS253YS_DB.APP_PRODUCTION.ANKEN_MEISAI (\\n  ID,\\n  DEPARTMENT_SHORT_NAME,\\n  SECTION_NAME,\\n  FISCAL_YEAR,\\n  PROJECT_NUMBER,\\n  BRANCH_NUMBER,\\n  SALES_CATEGORY,\\n  DEPARTMENT_ID,\\n  GROUP_SHORT_NAME,\\n  CUSTOMER_ID,\\n  CUSTOMER_NAME,\\n  ORDER_NUMBER,\\n  ORDER_NAME,\\n  SUBJECT,\\n  PROJECT_NAME,\\n  WORK_START_DATE,\\n  WORK_END_DATE,\\n  ACCOUNTING_MONTH,\\n  RANK,\\n  AMOUNT,\\n  SALES_DELIVERY_FLAG,\\n  INVOICE_NUMBER,\\n  ACTIVE_FLAG,\\n  CUSTOMER_QUOTE_REQUEST_NUMBER,\\n  CUSTOMER_ORDER_NUMBER,\\n  DIVISION_CODE,\\n  DEPARTMENT_NAME,\\n  DEPARTMENT_SECTION_SHORT_NAME\\n)\\nFROM @GBPS253YS_DB.APP_PRODUCTION.RAW_DATA\\nFILES = ('案件：案件明細一覧20251217203031_0.csv')\\nFILE_FORMAT = (\\n    TYPE=CSV,\\n    SKIP_HEADER=1,\\n    FIELD_DELIMITER=',',\\n    TRIM_SPACE=FALSE,\\n    FIELD_OPTIONALLY_ENCLOSED_BY = '\\"',\\n    REPLACE_INVALID_CHARACTERS=TRUE,\\n    DATE_FORMAT=AUTO,\\n    TIME_FORMAT=AUTO,\\n    TIMESTAMP_FORMAT=AUTO,\\n    EMPTY_FIELD_AS_NULL=TRUE,\\n    NULL_IF = (''),\\n    error_on_column_count_mismatch=false\\n)\\nON_ERROR=CONTINUE\\nFORCE = TRUE;\\n```\\n\\n##### 部署マスタ\\n\\n```sql\\nCOPY INTO GBPS253YS_DB.APP_PRODUCTION.DEPARTMENT_MASTER (\\n  ID,                         -- 1\\n  FISCAL_YEAR,                -- 2\\n  DEPARTMENT_CATEGORY,        -- 3\\n  DEPARTMENT_ID,              -- 4\\n  DIVISION_CODE,              -- 5 部門CD\\n  DEPARTMENT_SECTION_CODE,    -- 6 部課CD\\n  HEADQUARTERS_CODE,          -- 7 本部CD\\n  GENERAL_DEPARTMENT_CODE,    -- 8 統括部CD\\n  DEPARTMENT_CODE,            -- 9 部CD\\n  SECTION_CODE,               -- 10 課CD\\n  GROUP_CODE,                 -- 11 グループCD\\n  FULL_NAME,                  -- 12 正式名称\\n  SHORT_NAME,                 -- 13 略称\\n  COMBINED_NAME,              -- 14 組合せ名称\\n  COMBINED_SHORT_NAME,        -- 15 組合せ略称\\n  ACCOUNTING_DEPARTMENT_CODE  -- 16 経理部門CD\\n)\\nFROM @GBPS253YS_DB.APP_PRODUCTION.RAW_DATA\\nFILES = ('部署マスタ20251217205645_0.csv')\\nFILE_FORMAT = (\\n  TYPE=CSV,\\n  SKIP_HEADER=1,\\n  FIELD_DELIMITER=',',\\n  FIELD_OPTIONALLY_ENCLOSED_BY='\\"',\\n  REPLACE_INVALID_CHARACTERS=TRUE,\\n  DATE_FORMAT=AUTO,\\n  TIME_FORMAT=AUTO,\\n  TIMESTAMP_FORMAT=AUTO,\\n  EMPTY_FIELD_AS_NULL=TRUE,\\n  NULL_IF=(''),\\n  ERROR_ON_COLUMN_COUNT_MISMATCH=FALSE\\n)\\nON_ERROR=CONTINUE\\nFORCE=TRUE;\\n\\n```\\n\\n#### DIM_ENTITY_ALIASデータの生成\\n\\n```sql\\nINSERT OVERWRITE INTO NAME_RESOLUTION.DIM_ENTITY_ALIAS (\\n  alias_normalized,\\n  entity_type,\\n  alias_raw,\\n  confidence,\\n  entity_id,\\n  entity_name,\\n  is_active,\\n  priority,\\n  refresh_run_id,\\n  refreshed_at\\n)\\nSELECT\\n  alias_normalized,\\n  entity_type,\\n  alias_raw,\\n  confidence,\\n  entity_id,\\n  entity_name,\\n  is_active,\\n  priority,\\n  TO_VARCHAR(CURRENT_TIMESTAMP()) AS refresh_run_id,\\n  CURRENT_TIMESTAMP()             AS refreshed_at\\nFROM APP_PRODUCTION.V_ENTITY_ALIAS_ALL;\\n```\\n\\n\\n#### API実行ロールの作成\\n\\n```sql\\nUSE ROLE SECURITYADMIN;\\nCREATE OR REPLACE ROLE GBPS253YS_API_ROLE;\\n\\nUSE ROLE SECURITYADMIN;\\nCREATE OR REPLACE  USER GBPS253YS_API_USER\\n  LOGIN_NAME = 'GBPS253YS_API_USER'\\n  DISPLAY_NAME = 'GBPS253YS_API_USER'\\n  DEFAULT_ROLE = GBPS253YS_API_ROLE\\n  MUST_CHANGE_PASSWORD = FALSE;\\n\\nALTER USER GBPS253YS_API_USER SET DEFAULT_ROLE = GBPS253YS_API_ROLE;\\nALTER USER GBPS253YS_API_USER SET DEFAULT_WAREHOUSE = GBPS253YS_WH;\\nALTER USER GBPS253YS_API_USER SET DEFAULT_NAMESPACE = GBPS253YS_DB.APP_PRODUCTION;\\n\\nGRANT ROLE GBPS253YS_API_ROLE TO USER GBPS253YS_API_USER;\\n\\n-- 2) Network Policy（API_USERだけ）\\nCREATE OR REPLACE NETWORK POLICY GBPS253YS_API_ONLY\\n  ALLOWED_IP_LIST = ('4.189.129.1');\\n\\nALTER USER GBPS253YS_API_USER SET NETWORK_POLICY = GBPS253YS_API_ONLY;\\n\\n-- 3) Agent権限（オーナーロールで）\\nUSE ROLE ACCOUNTADMIN;\\n\\n-- 既存\\nGRANT USAGE ON WAREHOUSE GBPS253YS_WH TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON ALL SCHEMAS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON ALL TABLES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON ALL VIEWS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON ALL SEQUENCES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON ALL FUNCTIONS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON ALL PROCEDURES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON ALL AGENT IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON AGENT GBPS253YS_DB.APP_PRODUCTION.SNOWFLAKE_DEMO_AGENT TO ROLE GBPS253YS_API_ROLE;\\n\\n-- 将来\\nGRANT USAGE ON FUTURE SCHEMAS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON FUTURE TABLES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON FUTURE VIEWS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON FUTURE SEQUENCES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON FUTURE FUNCTIONS IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\nGRANT USAGE ON FUTURE PROCEDURES IN DATABASE GBPS253YS_DB TO ROLE GBPS253YS_API_ROLE;\\n\\n\\nGRANT READ ON STAGE GBPS253YS_DB.APP_PRODUCTION.RAW_DATA TO ROLE GBPS253YS_API_ROLE;\\nGRANT WRITE ON STAGE GBPS253YS_DB.APP_PRODUCTION.RAW_DATA TO ROLE GBPS253YS_API_ROLE;\\n\\n-- 4) PAT発行（表示される値を保存）\\nUSE ROLE SECURITYADMIN;\\n\\nALTER USER GBPS253YS_API_USER\\n  ADD PROGRAMMATIC ACCESS TOKEN azure_func_token;\\n```\\n\\n\\n# 参考）使用方法\\n\\n1. 以下のようにしてデータを生成する\\n```sql\\n-- =========================================================\\n-- Weekly manual profiling + evidence export + ingest\\n-- =========================================================\\n\\n-- 0) 実行日（YYYYMMDD_HH24MISSFF3）を自動で作る\\nSET RUN_DATE = TO_VARCHAR(\\n  DATEADD(\\n    day,\\n    -1,\\n    CONVERT_TIMEZONE('UTC', 'Asia/Tokyo', CURRENT_TIMESTAMP())\\n  ),\\n  'YYYYMMDD_HH24MISSFF3'\\n);\\n\\n-- 1) メトリクス収集（週1回）\\n--    ※ 業務データスキーマのみ（例：APP_PRODUCTION）\\nCALL DB_DESIGN.PROFILE_ALL_TABLES(\\n  'GBPS253YS_DB',\\n  'APP_PRODUCTION',\\n  10.0,\\n  'weekly manual profiling'\\n);\\n\\nCALL DB_DESIGN.PROFILE_ALL_TABLES(\\n  'GBPS253YS_DB',\\n  'DB_DESIGN',\\n  10.0,\\n  'weekly manual profiling'\\n);\\n\\n-- 2) メトリクス情報を Markdown (+ raw json) にして S3 更新\\n--    ※ DB_DESIGN.V_PROFILE_RESULTS_LATEST をソースにするのはOK（基盤側）\\nCALL DB_DESIGN.EXPORT_PROFILE_EVIDENCE_MD_VFINAL(\\n  'GBPS253YS_DB',              -- P_SOURCE_DB\\n  'DB_DESIGN',                -- P_SOURCE_SCHEMA\\n  'V_PROFILE_RESULTS_LATEST', -- P_SOURCE_VIEW\\n  'GBPS253YS_DB',              -- P_TARGET_DB (フィルタ用)\\n  $RUN_DATE,                  -- P_RUN_DATE（自動）\\n  'reviews/profiles',         -- P_VAULT_PREFIX\\n  'APP_PRODUCTION'                  -- P_TARGET_SCHEMA（業務スキーマのみ）\\n);\\nCALL DB_DESIGN.EXPORT_PROFILE_EVIDENCE_MD_VFINAL(\\n  'GBPS253YS_DB',              -- P_SOURCE_DB\\n  'DB_DESIGN',                -- P_SOURCE_SCHEMA\\n  'V_PROFILE_RESULTS_LATEST', -- P_SOURCE_VIEW\\n  'GBPS253YS_DB',              -- P_TARGET_DB (フィルタ用)\\n  $RUN_DATE,                  -- P_RUN_DATE（自動）\\n  'reviews/profiles',         -- P_VAULT_PREFIX\\n  'DB_DESIGN'                  -- P_TARGET_SCHEMA（業務スキーマのみ）\\n);\\n\\n-- 3) 最新の Markdown を取り込み（Vault全文でもOKだが絞ると軽い）\\n--    reviews/profiles 配下の md だけ取り込む例\\nCALL DB_DESIGN.INGEST_VAULT_MD(\\n  '@DB_DESIGN.OBSIDIAN_VAULT_STAGE',\\n  '.*\\\\.md'\\n);\\n```\\n\\n2. Snowflake Cortex Agentにレビュー依頼する。\\n\\n\\n\\n#### クリーニング\\n\\n```sql\\nDROP DATABASE GBPS253YS_DB;\\nDROP WAREHOUSE GBPS253YS_WH;\\n```\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: EXT_20251226181900\\ntable_id: TBL_20260102230002\\nphysical: FINISHED_AT\\ndomain: TIMESTAMP_LTZ\\npk: false\\nref_table_id:\\nref_column:\\nref_cardinality:\\nis_nullable: true\\ndefault: null\\ncomment: プロファイル実行終了時刻。実行中（STATUS='RUNNING'）はNULL、完了時（SUCCEEDED / FAILED）に設定される。 (外部テーブル版: EXT_PROFILE_RUNS)\\n---\\n\\n# FINISHED_AT\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: COL_20260102000307\\ntable_id: TBL_20260102000102\\nphysical: DURATION_MS\\ndomain: NUMBER\\npk: false\\nref_table_id:\\nref_column:\\nref_cardinality:\\nis_nullable: true\\ndefault: \\ncomment: 実行時間(ミリ秒)\\n---\\n\\n# DURATION_MS\\n"},{"count":1,"value":"---\\ntype: schema\\nschema_id: SCH_20251228235503\\nphysical: NAME_RESOLUTION\\ncomment: 名称解決（手動辞書）\\n---\\n\\n# NAME_RESOLUTION\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: EXT_20251226182318\\ntable_id: TBL_20260102230001\\nphysical: RUN_ID\\ndomain: VARCHAR\\npk: true\\nref_table_id:\\nref_column:\\nref_cardinality:\\nis_nullable: false\\ndefault: null\\ncomment: プロファイル実行（run）を識別するID。PROFILE_RUNS.RUN_ID と対応し、どの実行に紐づく結果かを示す。 (外部テーブル版: EXT_PROFILE_RESULTS)\\n---\\n\\n# RUN_ID\\n"},{"count":1,"value":"---\\ntype: other\\nschema_id: SCH_20251226180633\\nphysical: OBSIDIAN_VAULT_STAGE\\nobject_type: procedure\\ncomment:\\n---\\n\\n# SQL\\n```sql\\nCREATE OR REPLACE STAGE DB_DESIGN.OBSIDIAN_VAULT_STAGE\\n  URL = 's3://135365622922-snowflake-chatdemo-vault-prod/'\\n  STORAGE_INTEGRATION = S3_OBSIDIAN_INT;\\n```\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: COL_20251225151138\\ntable_id: TBL_20251225144134\\nphysical: DIVISION_CODE\\ndomain: VARCHAR\\npk: false\\nref_table_id:\\nref_column:\\nref_cardinality:\\nis_nullable: true\\ndefault: \\ncomment: 部門CD\\n---\\n\\n# DIVISION_CODE\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: COL_20251225135947\\ntable_id: TBL_20251225133349\\nphysical: SECTION_NAME\\ndomain: VARCHAR\\npk: false\\nref_table_id:\\nref_column:\\nref_cardinality:\\nis_nullable: true\\ndefault: \\ncomment: 課名（正式）\\n---\\n\\n# SECTION_NAME\\n"},{"count":1,"value":"---\\ntype: view\\nview_id: VW_20251229015446\\nschema_id: SCH_20251225131727\\nphysical: V_ENTITY_ALIAS_AUTO\\ncomment: 自動生成エイリアス辞書（部署/顧客/案件/オーダー）。手動辞書と統合して名称解決に利用する。\\n---\\n\\n# V_ENTITY_ALIAS_AUTO\\n\\n## View Columns\\n> ここは VIEW の括弧内定義（列名＋列コメント）を書く（型は不要）\\n\\n| column_name      | comment                                           |\\n| ---------------- | ------------------------------------------------- |\\n| ALIAS_RAW        | 別名（正規化前の生文字列）                                     |\\n| ALIAS_NORMALIZED | 正規化後の別名（NORMALIZE_JA / NORMALIZE_JA_DEPT 適用後）     |\\n| ENTITY_TYPE      | エンティティ種別（department / customer / project / order） |\\n| ENTITY_ID        | エンティティID（共通利用のため varchar）                         |\\n| ENTITY_NAME      | 正式名称（表示用）                                         |\\n| CONFIDENCE       | 別名の信頼度（0.0〜1.0）                                   |\\n| PRIORITY         | 優先度（小さいほど優先）                                      |\\n| IS_ACTIVE        | 有効フラグ                                              |\\n\\n## SQL\\n```sql\\n/* =========================\\n   部門：正式名称\\n   ========================= */\\nSELECT\\n  FULL_NAME                      AS alias_raw,\\n  NORMALIZE_JA_DEPT(FULL_NAME)   AS alias_normalized,\\n  'department'                   AS entity_type,\\n  ID::VARCHAR                    AS entity_id,\\n  FULL_NAME                      AS entity_name,\\n  1.00                           AS confidence,\\n  1000                           AS priority,\\n  TRUE                           AS is_active\\nFROM GBPS253YS_DB.APP_PRODUCTION.DEPARTMENT_MASTER\\nWHERE FULL_NAME IS NOT NULL\\n\\nUNION ALL\\n\\n/* =========================\\n   部門：略称\\n   ========================= */\\nSELECT\\n  SHORT_NAME                     AS alias_raw,\\n  NORMALIZE_JA_DEPT(SHORT_NAME)  AS alias_normalized,\\n  'department'                   AS entity_type,\\n  ID::VARCHAR                    AS entity_id,\\n  FULL_NAME                      AS entity_name,\\n  0.85                           AS confidence,\\n  1000                           AS priority,\\n  TRUE                           AS is_active\\nFROM GBPS253YS_DB.APP_PRODUCTION.DEPARTMENT_MASTER\\nWHERE SHORT_NAME IS NOT NULL\\n\\nUNION ALL\\n\\n/* =========================\\n   部門：複合正式名称（本部＋部門）\\n   ========================= */\\nSELECT\\n  COMBINED_NAME                      AS alias_raw,\\n  NORMALIZE_JA_DEPT(COMBINED_NAME)   AS alias_normalized,\\n  'department'                       AS entity_type,\\n  ID::VARCHAR                        AS entity_id,\\n  FULL_NAME                          AS entity_name,\\n  0.95                               AS confidence,\\n  1000                               AS priority,\\n  TRUE                               AS is_active\\nFROM GBPS253YS_DB.APP_PRODUCTION.DEPARTMENT_MASTER\\nWHERE COMBINED_NAME IS NOT NULL\\n\\nUNION ALL\\n\\n/* =========================\\n   部門：複合略称\\n   ========================= */\\nSELECT\\n  COMBINED_SHORT_NAME                      AS alias_raw,\\n  NORMALIZE_JA_DEPT(COMBINED_SHORT_NAME)   AS alias_normalized,\\n  'department'                             AS entity_type,\\n  ID::VARCHAR                              AS entity_id,\\n  FULL_NAME                                AS entity_name,\\n  0.80                                     AS confidence,\\n  1000                                     AS priority,\\n  TRUE                                     AS is_active\\nFROM GBPS253YS_DB.APP_PRODUCTION.DEPARTMENT_MASTER\\nWHERE COMBINED_SHORT_NAME IS NOT NULL\\n\\nUNION ALL\\n\\n/* =========================\\n   顧客（Customer）\\n   ========================= */\\nSELECT\\n  CUSTOMER_NAME                 AS alias_raw,\\n  NORMALIZE_JA(CUSTOMER_NAME)   AS alias_normalized,\\n  'customer'                    AS entity_type,\\n  CUSTOMER_ID::VARCHAR          AS entity_id,\\n  CUSTOMER_NAME                 AS entity_name,\\n  1.00                          AS confidence,\\n  1000                          AS priority,\\n  TRUE                          AS is_active\\nFROM GBPS253YS_DB.APP_PRODUCTION.V_CUSTOMER_MASTER\\n\\nUNION ALL\\n\\n/* =========================\\n   案件：案件名（project_name）\\n   ========================= */\\nSELECT\\n  PROJECT_NAME                 AS alias_raw,\\n  NORMALIZE_JA(PROJECT_NAME)   AS alias_normalized,\\n  'project'                    AS entity_type,\\n  PROJECT_NUMBER               AS entity_id,\\n  PROJECT_NAME                 AS entity_name,\\n  1.00                         AS confidence,\\n  1000                         AS priority,\\n  TRUE                         AS is_active\\nFROM GBPS253YS_DB.APP_PRODUCTION.V_PROJECT_MASTER\\nWHERE PROJECT_NAME IS NOT NULL\\n\\nUNION ALL\\n\\n/* =========================\\n   案件：件名（subject）\\n   ========================= */\\nSELECT\\n  SUBJECT                      AS alias_raw,\\n  NORMALIZE_JA(SUBJECT)        AS alias_normalized,\\n  'project'                    AS entity_type,\\n  PROJECT_NUMBER               AS entity_id,\\n  PROJECT_NAME                 AS entity_name,\\n  0.90                         AS confidence,\\n  1000                         AS priority,\\n  TRUE                         AS is_active\\nFROM GBPS253YS_DB.APP_PRODUCTION.V_PROJECT_MASTER\\nWHERE SUBJECT IS NOT NULL\\n\\nUNION ALL\\n\\n/* =========================\\n   オーダー（Order）\\n   ========================= */\\nSELECT\\n  ORDER_NAME                 AS alias_raw,\\n  NORMALIZE_JA(ORDER_NAME)   AS alias_normalized,\\n  'order'                    AS entity_type,\\n  ORDER_NUMBER::VARCHAR      AS entity_id,\\n  ORDER_NAME                 AS entity_name,\\n  1.00                       AS confidence,\\n  1000                       AS priority,\\n  TRUE                       AS is_active\\nFROM GBPS253YS_DB.APP_PRODUCTION.V_ORDER_MASTER\\n```\\n"},{"count":1,"value":"# [[design.RESOLVE_ENTITY_ALIAS]] 設計書\\n\\n## 概要\\n\\nRESOLVE_ENTITY_ALIASは、ユーザーが入力した固有名詞（顧客名、部署名、案件名、オーダー名等）をエンティティIDに決定論的に解決するストアドプロシージャである。曖昧なLIKE検索やLLM推測に頼らず、正規化テーブル（[[design.DIM_ENTITY_ALIAS]]）を用いた段階的検索により、確定的または候補提示のいずれかを返却する。\\n\\n## 業務上の意味\\n\\n自然言語データ分析では、ユーザーが「デジタルイノベーション部の売上を教えて」といった形で固有名詞を含む質問を投げる。従来のLIKE検索では表記揺れや部分一致により誤った候補を返したり、候補が多すぎて選択できなかったりする問題がある。本プロシージャは、正規化辞書を用いた決定論的な名称解決により、候補が1件に絞れる場合は自動確定し、複数候補がある場合はユーザーに選択を促すことで、誤った推測を排除する。\\n\\n## 設計上の位置づけ\\n\\nRESOLVE_ENTITY_ALIASはAPP_PRODUCTIONスキーマに配置され、以下のオブジェクトと連携する。\\n\\n- [[design.NORMALIZE_JA]]: 入力文字列の汎用正規化\\n- [[design.NORMALIZE_JA_DEPT]]: 部署名の特化正規化\\n- [[design.DIM_ENTITY_ALIAS]]: エイリアステーブル（NAME_RESOLUTIONスキーマ）\\n- [[design.RESOLVE_ENTITY_ALIAS_TOOL]]: Cortex Agent向けのラッパーツール\\n- [[design.SNOWFLAKE_DEMO_AGENT]]: 名称解決の結果をもとに集計やスコープ展開を実行\\n\\n本プロシージャは3段階の検索戦略（生の一致→正規化一致→正規化部分一致）を実施し、各段階で候補が1件のみの場合は自動確定、複数候補が残る場合はユーザーに選択を促す。\\n\\n## 機能\\n\\n1. 入力文字列の正規化\\n   - NORMALIZE_JAで汎用正規化\\n   - NORMALIZE_JA_DEPTで部署名特化正規化\\n\\n2. 3段階の検索戦略\\n   - Step 1: alias_rawによる完全一致（生の入力との一致）\\n   - Step 2: alias_normalizedによる完全一致（正規化後の一致）\\n   - Step 3: alias_normalizedによる部分一致（LIKE検索）\\n\\n3. 自動確定ロジック\\n   - 候補が1件のみの場合: resolved=候補、decided=true、next=\\"aggregate\\"\\n   - 候補が複数でトップのconfidenceが0.95以上、かつ2位との差が0.10以上の場合: resolved=トップ候補、decided=true、next=\\"aggregate\\"\\n\\n4. 候補提示ロジック\\n   - 上記以外の場合: resolved=null、decided=false、next=\\"disambiguate\\"、candidates=候補リスト\\n\\n5. エンティティタイプヒント対応\\n   - entity_type_hintパラメータが指定されている場合は、そのタイプのみを検索\\n   - 未指定の場合は全タイプから検索\\n\\n6. 優先度・信頼度による順序付け\\n   - priority昇順→confidence降順→alias_normalized長さ昇順でソート\\n   - 最大候補数はmax_candidatesパラメータで制御\\n\\n## パラメータ\\n\\n- term (STRING): 解決対象の固有名詞（必須）\\n- max_candidates (STRING): 候補上限（必須、文字列形式で渡す、通常\\"8\\"）\\n- entity_type_hint (STRING): タイプヒント（任意、'department'/'customer'/'project'/'order'または省略）\\n\\n## 利用シーン\\n\\n- 自然言語データ分析での固有名詞解決: ユーザーが「デジタルイノベーション部」と入力した場合に、`DEPARTMENT_ID`='D123'に確定\\n- 顧客名の名寄せ: 「ＡＢＣ株式会社」「ABC(株)」「エービーシー」といった表記揺れを同一顧客に解決\\n- 案件名・オーダー名の特定: 部分的な名称や略称から正しいプロジェクトやオーダーを特定\\n- 候補絞り込み: タイプヒントを指定することで、部署のみ・顧客のみといった限定検索を実現\\n- エージェント連携: Cortex Agentのツールとして呼び出され、名称解決結果をもとに集計やスコープ展開を実行\\n- データクリーニング: 重複エンティティの名寄せやマスタデータの正規化処理\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: COL_20251225151420\\ntable_id: TBL_20251225144134\\nphysical: COMBINED_SHORT_NAME\\ndomain: VARCHAR\\npk: false\\nref_table_id:\\nref_column:\\nref_cardinality:\\nis_nullable: true\\ndefault: \\ncomment: 組合せ略称\\n---\\n\\n# COMBINED_SHORT_NAME\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: COL_20260102000211\\ntable_id: TBL_20260102000101\\nphysical: DAY\\ndomain: NUMBER\\npk: false\\nref_table_id:\\nref_column:\\nref_cardinality:\\nis_nullable: false\\ndefault: \\ncomment: パーティション:日\\n---\\n\\n# DAY\\n"},{"count":1,"value":"# スキーマ設計：[[design.APP_PRODUCTION]]\\n\\n## 概要\\n[[design.APP_PRODUCTION]] スキーマは、デモ用途の「らくらく案件データ」を Snowflake 上で扱うための最小構成スキーマである。  \\n外部（主に CSV）から取り込んだデータを受け止める取込テーブル層と、取込データを元に整形・正規化して提供する VIEW 層を分離し、Analyst（セマンティックモデル）や Agent（Cortex Agent）から利用できる状態を作る。\\n\\n本スキーマにおける基本思想は以下：\\n- 取込の安定性を最優先（取込テーブルでは制約を強くしない）\\n- 参照・分析は VIEW を正本とする（生テーブル直参照を避ける）\\n- デモ手順（COPY / VIEW / YAML / Analyst / Agent）を再現可能な形で保持する\\n\\n## スキーマ内のレイヤ構成\\n\\n### 1) 取込（Raw / Landing）層\\nCSV をそのまま受け止める一次受け皿。  \\n形式揺れ、欠損、重複があってもロードを止めない設計判断を許容する。\\n\\n- 例：\\n  - [[design.ANKEN_MEISAI]]（案件明細：CSV取込用）\\n  - [[design.DEPARTMENT_MASTER]]（部署マスタ（年度別）：CSV取込用）\\n- 取込層では以下を「意図的に行わない」ことがある：\\n  - 主キー / 外部キー制約\\n  - NOT NULL 制約\\n  - 厳密な型制約（日付・コードの統一など）\\n- データ品質保証は後段（VIEW / 正規化層）の責務とする。\\n\\n### 2) 提供（Views / Normalized）層\\n取込層のデータを、参照・分析に適した粒度・型・意味に整形して提供する層。  \\nデモでは「正規化 VIEW 群」として実装し、Analyst のセマンティックモデルの入力として利用する。\\n\\n- 例：\\n  - [[design.V_CUSTOMER_MASTER]]（案件明細から生成する取引先マスタVIEW）\\n  - [[design.V_PROJECT_MASTER]]（案件番号＋枝番＋年度単位の案件マスタVIEW）\\n  - [[design.V_ORDER_MASTER]]（オーダ番号単位のオーダマスタVIEW）\\n  - [[design.V_INVOICE]]（請求番号×計上月単位、金額集計済のVIEW）\\n  - [[design.V_PROJECT_FACT]]（生データ粒度を保持するファクトVIEW）\\n\\n提供層は、取込層の内容を「直接更新」せず、参照用途のための投影・整形のみを行う。\\n\\n### 3) セマンティックモデル（YAML）層\\n提供層（VIEW 群）を元に、Analyst（cortex_analyst_text_to_sql）向けに  \\nディメンション／ファクト／メトリクス／リレーションシップを定義する層。\\n\\n- 目的：\\n  - 自然言語→SQL 変換を安定させる\\n  - 用語揺れ（同義語）を吸収する\\n  - JOIN の関係性を明示して誤結合を減らす\\n- 重要：\\n  - セマンティックモデルは「参照・分析の入口」であり、取込層を直接対象にしない運用を基本とする。\\n\\n### 4) Agent / API 連携層（デモ）\\nデモでは、Cortex Agent を Azure Functions から呼び出し、SWA（静的Web）でチャットUIを提供する。\\n\\n- Agent はスキーマ内データを検索・参照する設定（デモ用途）\\n- 実行ロール・ネットワークポリシー（PAT）等を用いて API 実行を制御する\\n\\n## 主要コンポーネントの関係（概念図）\\nCSV / 外部ファイル\\n  → （STAGE: [[design.RAW_DATA]]）\\n  → （COPY INTO）\\n  → 取込テーブル（[[design.ANKEN_MEISAI]] / [[design.DEPARTMENT_MASTER]]）\\n  → 提供 VIEW（V_*）\\n  → セマンティックモデル（YAML）\\n  → Analyst / Agent\\n  → Azure Functions API\\n  → SWA UI\\n\\n## 設計方針\\n\\n### 取込テーブルを「正本」にしない\\n取込テーブルは「来たものを保持する層」であり、\\n- 欠損や重複があり得る\\n- 表現揺れ（文字列日付、コード揺れ）があり得る\\nことを前提とする。\\n\\nそのため、参照・集計・検索は原則として VIEW 層を経由する。\\n\\n### VIEW 層で意味を確定させる\\nVIEW 層では、以下のような「意味の確定」を行う。\\n- 粒度の定義（案件・オーダ・請求など）\\n- 型変換（例：文字列日付 → DATE への変換）\\n- 集計や仮キーの補完（請求番号なしの場合のキー付与等）\\n\\nただし VIEW は「更新」ではなく「提供」であるため、取込層の原データは保持される。\\n\\n### キー設計の考え方（デモ前提）\\nデモでは、業務上の完全な参照整合や制約実装よりも、\\n- 分析で必要な JOIN が成立すること\\n- 粒度がぶれないこと\\n- 欠損があっても破綻しないこと\\nを優先する。\\n\\nそのため、自然キー（例：案件番号＋枝番＋年度）や仮キー（請求番号なしの請求キー）を VIEW 側で組み立てることがある。\\n\\n## 運用上の注意\\n- 取込テーブルへの直接参照を前提に BI / アプリケーションを構築しない\\n- 参照先は VIEW（正規化 VIEW 群）を基本とする\\n- COPY 実行は `ON_ERROR`=CONTINUE 等により「止めない」運用を許容する（デモ前提）\\n- セマンティックモデルは VIEW を入力として設計し、同義語・JOIN 関係を明示する\\n\\n## 将来拡張の余地\\n- 取込管理の高度化（取込実行ID、エラーログ、ハッシュ等の付与）\\n- VIEW 層からテーブル（正本）層への昇格（SCD、有効期間、整合性制約の導入）\\n- スキーマ内の命名規約・レイヤ規約の明文化（RAW / V_ / MART 等）\\n\\n## 関連\\n- テーブル設計：[[design.ANKEN_MEISAI]]\\n- テーブル設計：[[design.DEPARTMENT_MASTER]]\\n- デモ手順書：本ノート作成時点の手順書（Snowflake設定 / Azure Functions / SWA）\\n\\n---\\n\\n\\n\\nschema_id:: SCH_20251225131727\\n\\n```dataview\\nTABLE comment, physical\\nFROM \\"master/tables\\"\\nWHERE schema_id = this.schema_id\\nSORT schema_id, physical\\n```\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: COL_20251225142946\\ntable_id: TBL_20251225133349\\nphysical: SALES_CATEGORY\\ndomain: VARCHAR\\npk: false\\nref_table_id:\\nref_column:\\nref_cardinality:\\nis_nullable: true\\ndefault: \\ncomment: 売上区分\\n---\\n\\n# SALES_CATEGORY\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: COL_20251225134310\\ntable_id: TBL_20251225133349\\nphysical: DEPARTMENT_SHORT_NAME\\ndomain: VARCHAR\\npk: false\\nref_table_id:\\nref_column:\\nref_cardinality:\\nis_nullable: true\\ndefault:\\ncomment: 部名（略称）\\n---\\n\\n# DEPARTMENT_SHORT_NAME\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: COL_20251225143008\\ntable_id: TBL_20251225133349\\nphysical: DEPARTMENT_ID\\ndomain: VARCHAR\\npk: false\\nis_nullable: true\\ndefault:\\ncomment: 部署ID\\nref_table_id:\\nref_column:\\nref_cardinality:\\n---\\n\\n# DEPARTMENT_ID\\n"},{"count":1,"value":"<%*\\n/**\\n * Schema template (Snowflake)\\n * - schema_id: SCH_YYYYMMDDHHmmss\\n * - filename: <physical>.md\\n * - moves file to master/schemas/\\n */\\nconst folder = \\"master/schemas\\";\\n\\nfunction safeFileName(name){\\n  return String(name ?? \\"SCHEMA\\")\\n    .trim()\\n    .replaceAll(\\"/\\", \\"_\\")\\n    .replaceAll(\\"\\\\\\\\\\", \\"_\\")\\n    .replaceAll(\\":\\", \\"_\\");\\n}\\n\\nconst schemaId = \\"SCH_\\" + tp.date.now(\\"YYYYMMDDHHmmss\\");\\nconst physicalRaw = await tp.system.prompt(\\"物理名（例: PUBLIC）\\");\\nconst physical = safeFileName(physicalRaw);\\nconst comment  = await tp.system.prompt(\\"comment（任意）\\") ?? \\"\\";\\n\\n// 配置先＋ファイル名を physical にする\\nawait tp.file.move(`${folder}/${physical}`);\\n-%>\\n---\\ntype: schema\\nschema_id: <%- schemaId %>\\nphysical: <%- physicalRaw %>\\ncomment: <%- comment %>\\n---\\n\\n# <%- physicalRaw %>\\n"},{"count":1,"value":"# 外部テーブル設計：[[design.EXT_PROFILE_RESULTS]]\\n\\n## 概要\\n[[DB_DESIGN.EXT_PROFILE_RESULTS]] は、データベーステーブルの各カラムに対して算出されたプロファイル計測結果をS3上のJSONLファイルとして保持し、Snowflakeから外部テーブルとして直接参照するテーブルである。  \\n1行が「1回のプロファイル実行（run）」における「1カラム分の計測結果」を表し、  \\n[[DB_DESIGN.PROFILE_RUNS.RUN_ID]] を起点として、対象テーブル・対象カラム・計測時点・計測結果を紐づける。\\n\\n本テーブルは、プロファイル処理の結果を外部ストレージに永続化し、品質確認・比較・監査・設計レビューの根拠として利用される。内部テーブル版（[[design.PROFILE_RESULTS]]）と同一の論理構造を持ちながら、S3直接参照によるストレージコスト最適化と長期保存を実現する。\\n\\n## 業務上の意味\\n- このテーブルが表す概念  \\n  - 「プロファイル結果（column-level metrics）」の外部永続化蓄積。\\n  - 1行は、ある run における、ある 1カラムの計測結果を表す。\\n  - 計測対象は  \\n    [[DB_DESIGN.PROFILE_RESULTS.TARGET_DB]] / [[DB_DESIGN.PROFILE_RESULTS.TARGET_SCHEMA]] / [[DB_DESIGN.PROFILE_RESULTS.TARGET_TABLE]] / [[DB_DESIGN.PROFILE_RESULTS.TARGET_COLUMN]]  \\n    により特定される。\\n- 主な利用シーン  \\n  - 長期データ品質トレンド分析（過去数ヶ月〜数年の比較）\\n  - 監査・コンプライアンス対応における証跡データとしての参照\\n  - コスト効率を重視した大量プロファイル結果の保存\\n  - 定期的な品質レポート生成の基礎データ\\n\\n## 設計上の位置づけ\\n\\n### 内部テーブル版との関係\\n本外部テーブルは [[DB_DESIGN.PROFILE_RESULTS]] と論理的に同一構造を持つ。\\n\\n- 論理設計の共通性  \\n  - カラム構成、データ型、主キーの考え方は内部テーブル版と同一とする。\\n  - [[DB_DESIGN.PROFILE_RESULTS.RUN_ID]] と [[DB_DESIGN.PROFILE_RESULTS.TARGET_COLUMN]] の複合キーにより、1run・1カラムあたり1行を前提とする。\\n  - METRICS 構造は VARIANT 型であり、将来拡張を許容する。\\n\\n- 物理配置の違い  \\n  - 内部テーブル：Snowflake管理ストレージに格納、高速検索・集計に最適化\\n  - 外部テーブル：S3直接参照、ストレージコスト最適化・長期保存に適する\\n\\n### 使い分けの方針\\n- 内部テーブル（[[design.PROFILE_RESULTS]]）を使うべきケース  \\n  - 頻繁なクエリ実行（日次監視、リアルタイム分析）\\n  - 複雑な JOIN や集計が必要な分析処理\\n  - 低レイテンシが要求される運用監視\\n  \\n- 外部テーブル（[[design.EXT_PROFILE_RESULTS]]）を使うべきケース  \\n  - 長期保存が主目的（過去1年以上のデータ）\\n  - 参照頻度が低い履歴データの保管\\n  - ストレージコストの削減が優先される状況\\n  - 外部システムとのデータ共有（S3経由）\\n\\n## 設計方針\\n\\n### S3統合設計\\n- ステージ名：[[design.OBSIDIAN_VAULT_STAGE]]  \\n  S3バケット `s3://snowflake-chatdemo-vault-prod/` への外部ステージを前提とする。\\n\\n- ファイルフォーマット：FF_JSON_LINES  \\n  1行1JSONのJSON Lines形式（NDJSON）を採用し、ストリーミング書き込みと部分読み取りの効率化を図る。\\n\\n- S3パス構造：  \\n  ```\\n  s3://snowflake-chatdemo-vault-prod/profile_results/year=YYYY/month=MM/day=DD/\\n  ```\\n  - プロファイル実行日時（STARTED_AT または書き込み日時）を基準としたパーティション構造とする。\\n  - 年月日によるパーティショニングにより、クエリ時のスキャン範囲を限定し、検索効率を向上させる。\\n\\n### パーティショニング戦略\\n- パーティションキー：year / month / day  \\n  実行日時に基づく時系列パーティションを採用する。\\n\\n- 設計理由  \\n  - プロファイル結果の参照は「特定期間のデータ品質変化」を追跡するケースが多い。\\n  - 日次パーティションにより、必要な期間のデータのみをスキャン対象とすることで、クエリコストを削減できる。\\n  - S3上のファイル配置とSnowflakeのパーティションプルーニングが連動し、不要なファイル読み取りを回避する。\\n\\n- パーティション粒度の考慮  \\n  - 日次（day）パーティションは、典型的なプロファイル実行頻度（日次バッチ）と整合する。\\n  - 月次パーティションでは粒度が粗すぎ、時間単位パーティションでは細かすぎる可能性がある。\\n  - 実運用データ量が増加した場合、時間単位への細分化を検討する余地がある。\\n\\n### 自動リフレッシュ\\n- auto_refresh: false  \\n  手動またはスケジュールによるリフレッシュを前提とし、自動リフレッシュは無効化する。\\n\\n- 設計理由  \\n  - プロファイル結果は日次バッチ等で定期的に書き込まれることを想定する。\\n  - 自動リフレッシュは予期しないコスト増加を招く可能性があるため、明示的な制御を優先する。\\n  - 必要に応じて `ALTER EXTERNAL TABLE ... REFRESH` を実行する運用とする。\\n\\n## コスト vs 性能のトレードオフ\\n\\n### ストレージコスト\\n- 外部テーブル（S3）：低コスト  \\n  S3ストレージ単価はSnowflake管理ストレージより大幅に安価である。\\n  長期保存が必要なプロファイル履歴データにおいて、外部テーブル化によるコスト削減効果は大きい。\\n\\n- 内部テーブル：高コスト  \\n  Snowflake管理ストレージは高可用性・高性能を提供する反面、単価が高い。\\n  数年分の履歴データを内部テーブルに保持すると、ストレージコストが増大する。\\n\\n### クエリ性能\\n- 外部テーブル：低速〜中速  \\n  - S3からのファイル読み取りはネットワーク経由となり、レイテンシが発生する。\\n  - パーティションプルーニングが効果的に働く場合、性能劣化は限定的である。\\n  - 複雑なJOINや集計処理では、内部テーブルに比べて性能が劣る可能性がある。\\n\\n- 内部テーブル：高速  \\n  - Snowflake管理ストレージは最適化されており、低レイテンシでのアクセスが可能。\\n  - マイクロパーティション・クラスタリングにより、高速な検索・集計が実現される。\\n\\n### 運用の考え方\\n- 初期段階では内部テーブル（[[design.PROFILE_RESULTS]]）にデータを蓄積し、定期的に外部テーブル（[[design.EXT_PROFILE_RESULTS]]）へアーカイブする運用を推奨する。\\n- 直近数ヶ月のデータは内部テーブルで高速参照を実現し、過去データは外部テーブルでコスト効率的に保存する。\\n- 必要に応じて、内部テーブルと外部テーブルを統合するビュー（UNION ALL）を定義し、アプリケーションからは統一的にアクセスできるようにする。\\n\\n## 運用上の注意点\\n\\n### データ書き込み\\n- 外部テーブルへの直接 INSERT / UPDATE / DELETE は不可である。\\n- S3へのデータ書き込みは、以下のいずれかの方法で行う。\\n  - COPY INTO コマンドによる内部テーブルから外部ステージへのエクスポート\\n  - 外部プロセス（Azure Functions、AWS Lambda等）によるS3直接書き込み\\n  - Snowpipe による継続的なファイル取り込み（リアルタイム要件がある場合）\\n\\n### メタデータリフレッシュ\\n- 新規ファイルがS3に追加された場合、外部テーブルのメタデータをリフレッシュする必要がある。\\n  ```sql\\n  ALTER EXTERNAL TABLE DB_DESIGN.EXT_PROFILE_RESULTS REFRESH;\\n  ```\\n- リフレッシュは定期的なスケジュール実行（Task）または手動実行により行う。\\n\\n### パーティション管理\\n- パーティションフォルダ（year=YYYY/month=MM/day=DD/）の命名規則を厳守する。\\n- 誤ったパーティション構造でファイルを配置すると、クエリ時のパーティションプルーニングが機能せず、性能劣化やコスト増加を招く。\\n\\n### データ品質担保\\n- 外部テーブルは Snowflake の制約（PRIMARY KEY、FOREIGN KEY、CHECK）を持たない。\\n- データ品質は書き込み側のプロセス（プロシージャ、外部アプリケーション）で担保する必要がある。\\n- 不正なデータがS3に書き込まれた場合、外部テーブルからの参照時にエラーやNULL値として扱われる可能性がある。\\n\\n## 将来拡張の余地\\n- パーティション粒度の最適化（時間単位への細分化、または週次・月次への集約）\\n- 圧縮形式の見直し（Parquet、ORC等への変更によるストレージ効率向上）\\n- 内部テーブルとの自動同期機構（Task + Stream による定期アーカイブ）\\n- 外部テーブル用マテリアライズドビューの検討（頻繁な集計クエリの高速化）\\n- S3ライフサイクルポリシーとの連携（古いパーティションのGlacier移行）\\n\\n## 関連\\n\\n- 内部テーブル版：[[DB_DESIGN.PROFILE_RESULTS]]\\n- 関連外部テーブル：[[DB_DESIGN.EXT_PROFILE_RUNS]]\\n- 関連プロシージャ：[[DB_DESIGN.PROFILE_TABLE]], [[DB_DESIGN.PROFILE_ALL_TABLES]]\\n- マスター定義：[[DB_DESIGN.EXT_PROFILE_RESULTS]]（master/externaltables/）\\n"},{"count":1,"value":"---\\ntype: schema\\nschema_id: SCH_20251228235404\\nphysical: APP_DEVELOPMENT\\ncomment: アプリケーション開発\\n---\\n\\n# APP_DEVELOPMENT\\n"},{"count":1,"value":"---\\ntype: column\\ncolumn_id: COL_20260102000401\\ntable_id: TBL_20260102000103\\nphysical: LOG_ID\\ndomain: VARCHAR\\npk: false\\nref_table_id:\\nref_column:\\nref_cardinality:\\nis_nullable: false\\ndefault: \\ncomment: ログID\\n---\\n\\n# LOG_ID\\n"}]},"run_id":"RUN-3502f0d4-014a-46fc-bf64-d5733286f9d0"},{"as_of_at":"2026-01-02 00:39:13.697 -0800","column":"FILE_LAST_MODIFIED","metrics":{"distinct_count":0,"null_count":25,"null_rate":1.000000000000000e+00,"row_count":25,"top_values":[]},"run_id":"RUN-3502f0d4-014a-46fc-bf64-d5733286f9d0"},{"as_of_at":"2026-01-02 00:39:14.560 -0800","column":"FOLDER","metrics":{"distinct_count":5,"distinct_rate_non_null":1.612903225806452e-01,"max_len":20,"max_varchar":"views","min_len":5,"min_varchar":"MAINTENANCE_GUIDE.md","null_count":0,"null_rate":0.000000000000000e+00,"row_count":31,"top_values":[{"count":22,"value":"master"},{"count":5,"value":"design"},{"count":2,"value":"templates"},{"count":1,"value":"MAINTENANCE_GUIDE.md"},{"count":1,"value":"views"}]},"run_id":"RUN-3502f0d4-014a-46fc-bf64-d5733286f9d0"},{"as_of_at":"2026-01-02 00:39:15.553 -0800","column":"INGESTED_AT","metrics":{"distinct_count":22,"distinct_rate_non_null":1.000000000000000e+00,"max_len":29,"max_varchar":"2026-01-01 23:57:38.983 -0800","min_len":29,"min_varchar":"2026-01-01 23:54:16.757 -0800","null_count":0,"null_rate":0.000000000000000e+00,"row_count":22,"top_values":[{"count":1,"value":"2026-01-01 23:55:44.618 -0800"},{"count":1,"value":"2026-01-01 23:56:28.279 -0800"},{"count":1,"value":"2026-01-01 23:54:46.791 -0800"},{"count":1,"value":"2026-01-01 23:56:48.901 -0800"},{"count":1,"value":"2026-01-01 23:56:12.210 -0800"},{"count":1,"value":"2026-01-01 23:56:17.088 -0800"},{"count":1,"value":"2026-01-01 23:56:47.190 -0800"},{"count":1,"value":"2026-01-01 23:55:09.127 -0800"},{"count":1,"value":"2026-01-01 23:54:16.757 -0800"},{"count":1,"value":"2026-01-01 23:55:13.811 -0800"},{"count":1,"value":"2026-01-01 23:56:37.902 -0800"},{"count":1,"value":"2026-01-01 23:55:02.037 -0800"},{"count":1,"value":"2026-01-01 23:55:01.022 -0800"},{"count":1,"value":"2026-01-01 23:57:04.947 -0800"},{"count":1,"value":"2026-01-01 23:56:57.370 -0800"},{"count":1,"value":"2026-01-01 23:55:00.158 -0800"},{"count":1,"value":"2026-01-01 23:54:47.646 -0800"},{"count":1,"value":"2026-01-01 23:55:05.633 -0800"},{"count":1,"value":"2026-01-01 23:55:35.008 -0800"},{"count":1,"value":"2026-01-01 23:55:48.641 -0800"}]},"run_id":"RUN-3502f0d4-014a-46fc-bf64-d5733286f9d0"},{"as_of_at":"2026-01-02 00:39:16.451 -0800","column":"OBJECT_ID","metrics":{"distinct_count":19,"distinct_rate_non_null":1.000000000000000e+00,"max_len":18,"max_varchar":"TBL_20260102230001","min_len":18,"min_varchar":"COL_20251225142946","null_count":8,"null_rate":2.962962962962963e-01,"row_count":27,"top_values":[{"count":1,"value":"COL_20251226182047"},{"count":1,"value":"COL_20251225142946"},{"count":1,"value":"COL_20260102000509"},{"count":1,"value":"COL_20251229021241"},{"count":1,"value":"COL_20251225144212"},{"count":1,"value":"COL_20251225151338"},{"count":1,"value":"TBL_20260102230001"},{"count":1,"value":"EXT_20251226181525"},{"count":1,"value":"COL_20251226181239"},{"count":1,"value":"SCH_20260102000001"},{"count":1,"value":"EXT_20251226183651"},{"count":1,"value":"COL_20251226183651"},{"count":1,"value":"EXT_20251226181216"},{"count":1,"value":"COL_20251226182021"},{"count":1,"value":"SCH_20251225131727"},{"count":1,"value":"EXT_20251226182008"},{"count":1,"value":"COL_20251226181139"},{"count":1,"value":"SCH_20251226180633"},{"count":1,"value":"COL_20260102000204"}]},"run_id":"RUN-3502f0d4-014a-46fc-bf64-d5733286f9d0"},{"as_of_at":"2026-01-02 00:39:17.347 -0800","column":"OBJECT_TYPE","metrics":{"distinct_count":8,"distinct_rate_non_null":3.636363636363636e-01,"max_len":16,"max_varchar":"view","min_len":4,"min_varchar":"application","null_count":7,"null_rate":2.413793103448276e-01,"row_count":29,"top_values":[{"count":15,"value":"column"},{"count":1,"value":"schema"},{"count":1,"value":"semantic_view"},{"count":1,"value":"other"},{"count":1,"value":"view"},{"count":1,"value":"application"},{"count":1,"value":"profile_evidence"},{"count":1,"value":"table"}]},"run_id":"RUN-3502f0d4-014a-46fc-bf64-d5733286f9d0"},{"as_of_at":"2026-01-02 00:39:18.318 -0800","column":"PATH","metrics":{"distinct_count":36,"distinct_rate_non_null":1.000000000000000e+00,"max_len":71,"max_varchar":"views/tables.md","min_len":9,"min_varchar":"README.md","null_count":0,"null_rate":0.000000000000000e+00,"row_count":36,"top_values":[{"count":1,"value":"master/columns/APP_PRODUCTION.ANKEN_MEISAI.DEPARTMENT_ID.md"},{"count":1,"value":"master/columns/APP_PRODUCTION.ANKEN_MEISAI.WORK_END_DATE.md"},{"count":1,"value":"master/columns/APP_PRODUCTION.ANKEN_MEISAI.WORK_START_DATE.md"},{"count":1,"value":"master/columns/DB_DESIGN.PROFILE_RESULTS.TARGET_TABLE.md"},{"count":1,"value":"reviews/profiles/20251229_140447305/APP_PRODUCTION/DEPARTMENT_MASTER.md"},{"count":1,"value":"master/externaltables/LOG.AZSWA_LOGS.md"},{"count":1,"value":"master/columns/DB_DESIGN.DOCS_OBSIDIAN.CONTENT.md"},{"count":1,"value":"master/columns/DB_DESIGN.PROFILE_RUNS.ROLE_NAME.md"},{"count":1,"value":"master/columns/APP_PRODUCTION.DEPARTMENT_MASTER.COMBINED_NAME.md"},{"count":1,"value":"master/other/DB_DESIGN.INGEST_VAULT_MD.md"},{"count":1,"value":"master/views/APP_PRODUCTION.V_PROJECT_FACT.md"},{"count":1,"value":"reviews/profiles/20251229_140447305/DB_DESIGN/PROFILE_RESULTS.md"},{"count":1,"value":"reviews/schemas/DB_DESIGN_20260102_092948.md"},{"count":1,"value":"master/columns/APP_PRODUCTION.ANKEN_MEISAI.CUSTOMER_ORDER_NUMBER.md"},{"count":1,"value":"master/columns/NAME_RESOLUTION.DIM_ENTITY_ALIAS_MANUAL.ALIAS_RAW.md"},{"count":1,"value":"README.md"},{"count":1,"value":"master/columns/DB_DESIGN.EXT_PROFILE_RUNS.TARGET_TABLE.md"},{"count":1,"value":"master/columns/LOG.CORTEX_CONVERSATIONS.METADATA.md"},{"count":1,"value":"master/columns/APP_PRODUCTION.DEPARTMENT_MASTER.DEPARTMENT_CATEGORY.md"},{"count":1,"value":"master/columns/DB_DESIGN.PROFILE_RUNS.TARGET_DB.md"}]},"run_id":"RUN-3502f0d4-014a-46fc-bf64-d5733286f9d0"}],"run_date":"20260101_093816046","target_db":"GBPS253YS_DB","target_schema":"DB_DESIGN","target_table":"DOCS_OBSIDIAN"}
